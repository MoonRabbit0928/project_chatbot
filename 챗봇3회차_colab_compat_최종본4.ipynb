{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e97d212",
      "metadata": {
        "id": "8e97d212"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e9a254ab",
      "metadata": {
        "id": "e9a254ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "outputId": "7eab4d66-4279-4b73-eb8f-0742f30f3a88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6bf58df6-bf17-4164-9d7a-2fac5900173d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6bf58df6-bf17-4164-9d7a-2fac5900173d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ChatBotData_split.csv to ChatBotData_split.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "74e33786",
      "metadata": {
        "id": "74e33786"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = pos / tf.pow(10000, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
        "        return angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            pos=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model)\n",
        "\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        return pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "\n",
        "        self.dense = layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, attention_mask=None, training=False):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)   #입력 q, k, v에 대해 Dense 통과시켜줌.\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  #그리고 각각을 멀티헤드 형태로 쪼갬.\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, attention_mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
        "        output = self.dense(concat_attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation='relu'),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        ## 층 정규화\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, mask=None, training=False):\n",
        "        attn_output = self.mha(x, x, x, attention_mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation='relu'),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        self.dropout3 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n",
        "        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, mask=None, training=False):\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for enc_layer in self.enc_layers:\n",
        "            x = enc_layer(x, mask=mask, training=training)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for dec_layer in self.dec_layers:\n",
        "            x = dec_layer(x, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask, training=training)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "        self.final_layer = layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        inp, tar = inputs\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
        "        dec_output = self.decoder(tar, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask, training=training)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b69fa8",
      "metadata": {
        "id": "14b69fa8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4dd5980",
      "metadata": {
        "id": "c4dd5980"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ed4faf4",
      "metadata": {
        "id": "2ed4faf4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3a22176c",
      "metadata": {
        "id": "3a22176c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1102d43b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1102d43b",
        "outputId": "8a8f535d-8b06-4081-a742-6ba9d4757166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문의 결측치 개수\n",
            "['너 좋아하는 차 종류 있어 ?', 'ㅋㅋ 마시는 차 말한 거야 !', '완전 곡물류 좋아하네 ㅋㅋ', '그럼 오래 걸리지 않아 ?', '근데 냉침 하는 것도 귀찮겠다 ㅜㅠ'] 질문 개수:  100786 \n",
            "\n",
            "['무슨 차 ?  자동차 ?  마시는 차 ?', '아하 나 둥글레 ,  옥수수 ,  보리차 좋아해', '야쓰 끓이기 귀찮아서 냉침해 먹어', '끓이는 것보다는 훨씬 오래 걸리지 ㅠ', '응 !  그래서 매일은 안 먹고 가끔 마셔'] 질문 개수:  100786 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "train_data = pd.read_csv(\"ChatBotData_split.csv\", encoding = 'utf-8')\n",
        "train_data = train_data.dropna()\n",
        "train_data = train_data.drop_duplicates(subset = ['res'])\n",
        "\n",
        "print('질문의 결측치 개수'.format(train_data.isnull().sum()))\n",
        "\n",
        "questions = []\n",
        "for sentence in train_data['req']:\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    questions.append(sentence)\n",
        "\n",
        "answers = []\n",
        "for sentence in train_data['res']:\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    answers.append(sentence)\n",
        "\n",
        "print(questions[:5], '질문 개수: ',len(questions),'\\n')\n",
        "print(answers[:5], '질문 개수: ',len(questions),'\\n')\n",
        "\n",
        "questions = questions[:60000]\n",
        "answers = answers[:60000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "a3328df8",
      "metadata": {
        "id": "a3328df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d29b83-6eed-4046-95f6-f43344833769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(len(questions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2d654aa0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d654aa0",
        "outputId": "429db6a0-a1a5-444e-f1d3-a942e5f98037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "시작 토큰 번호 : [8142]\n",
            "종료 토큰 번호 : [8143]\n",
            "단어 집합의 크기 : 8144\n",
            "다른 지역으로 가는 것도 많이 생기면 좋겠다 ㅜㅠ\n"
          ]
        }
      ],
      "source": [
        "# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
        "\n",
        "\n",
        "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "#리스트 형태로 만들어둔 이유는 나중에 input + START_TOKEN + END_TOKEN 같이 쉽게 붙이기 위해서.\n",
        "\n",
        "\n",
        "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "\n",
        "print('시작 토큰 번호 :',START_TOKEN)\n",
        "print('종료 토큰 번호 :',END_TOKEN)\n",
        "print('단어 집합의 크기 :',VOCAB_SIZE)\n",
        "\n",
        "print(questions[20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1154b60",
      "metadata": {
        "id": "a1154b60"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e99b57",
      "metadata": {
        "id": "84e99b57"
      },
      "outputs": [],
      "source": [
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67dff197",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67dff197",
        "outputId": "a932e7e9-ea49-4ebf-c237-d45e71ba365e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문 최대 길이: 62\n",
            "질문 평균 길이: 9.857816666666666\n",
            "응답 최대 길이: 62\n",
            "응답 평균 길이: 9.293883333333333\n",
            "군인들에 대한 보상이 특히 약한 이유가 박정희 정권 때 베트남 파병 많이 갔었잖아 그래서 거기서 부상 당하거나 죽은 사람들이 많은데 그거 다  보상 해주려니까 돈이 너무 많이 들어서 법을 그  따위로 만들었다던데 진짜 나쁜 의미로 대단한 대통령이여 으휴\n"
          ]
        }
      ],
      "source": [
        "question_token_lens = [len(tokenizer.encode(q)) for q in questions]\n",
        "answer_token_lens = [len(tokenizer.encode(a)) for a in answers]\n",
        "\n",
        "print(\"질문 최대 길이:\", max(question_token_lens))\n",
        "print(\"질문 평균 길이:\", sum(question_token_lens) / len(question_token_lens))\n",
        "print(\"응답 최대 길이:\", max(answer_token_lens))\n",
        "print(\"응답 평균 길이:\", sum(answer_token_lens) / len(answer_token_lens))\n",
        "\n",
        "for q,a in zip(questions,answers):\n",
        "    lens_a = len(tokenizer.encode(a))\n",
        "    lens_q = len(tokenizer.encode(q))\n",
        "    #if lens_q == 64:\n",
        "       # print(q)\n",
        "    if lens_a == 62:\n",
        "       print(a)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31c6df06",
      "metadata": {
        "id": "31c6df06"
      },
      "outputs": [],
      "source": [
        "for i,a in enumerate(answers):\n",
        "    lens_a = len(tokenizer.encode(a))\n",
        "    if lens_a == 65:\n",
        "        print('인덱스: {}\\n, 질문: {}\\n, 답변: {}\\n\\n'.format(i, questions[i],a))\n",
        "    else:\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c0130c32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0130c32",
        "outputId": "2e3af6db-2c5c-4e90-d188-581e6127df23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문 데이터의 크기(shape) : (60000, 63)\n",
            "답변 데이터의 크기(shape) : (60000, 63)\n"
          ]
        }
      ],
      "source": [
        "# 최대 길이를 40으로 정의\n",
        "MAX_LENGTH = 63\n",
        "\n",
        "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    tokenized_inputs.append(sentence1)\n",
        "    tokenized_outputs.append(sentence2)\n",
        "\n",
        "  # 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "\n",
        "\n",
        "questions, answers = tokenize_and_filter(questions, answers)\n",
        "\n",
        "\n",
        "print('질문 데이터의 크기(shape) :', questions.shape)\n",
        "print('답변 데이터의 크기(shape) :', answers.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "759c7f03",
      "metadata": {
        "id": "759c7f03"
      },
      "outputs": [],
      "source": [
        "#데이터를 학습 데이터, 검증 데이터, 시험 데이터로 구분하여 텐서플로우의 데이터셋 형성\n",
        "q_train, q_v_t, a_train, a_v_t = train_test_split(questions, answers, test_size=0.2, random_state=42)\n",
        "\n",
        "q_val, q_test, a_val, a_test = train_test_split(q_v_t, a_v_t, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_inputs_train = a_train[:, :-1]\n",
        "decoder_inputs_val = a_val[:, :-1]\n",
        "decoder_inputs_test = a_test[:, :-1]\n",
        "\n",
        "# 디코더의 정답은 첫 번째 토큰을 제외한 것 (한 칸 오른쪽으로 shift)\n",
        "decoder_targets_train = a_train[:, 1:]\n",
        "decoder_targets_val = a_val[:, 1:]\n",
        "decoder_targets_test = a_test[:, 1:]\n",
        "#encoder_input, decoder_input, decoder_target 이 트리플 세트로 묶여서, 각 문장(샘플)마다 하나씩 학습이 진행되는 구조야.\n",
        "\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((\n",
        "    (q_train, decoder_inputs_train),  # input = (encoder_input, decoder_input)\n",
        "    decoder_targets_train\n",
        "))\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((\n",
        "    (q_val, decoder_inputs_val),  # input = (encoder_input, decoder_input)\n",
        "    decoder_targets_val\n",
        "))\n",
        "\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((\n",
        "    (q_test, decoder_inputs_test),  # input = (encoder_input, decoder_input)\n",
        "    decoder_targets_test\n",
        "))\n",
        "BATCH_SIZE = 64  #일단 컴퓨터가 맛탱이 갈 수도 있으니까 32로 ㄱㄱ\n",
        "BUFFER_SIZE = 10000  #일반적으로는 데이터 전체 수 이상을 쓰는 게 좋다네요. 예..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset_train = dataset_train.shuffle(BUFFER_SIZE)  #shuffle: 학습 데이터만 섞어줘야 일반화에 효과 있음.\n",
        "dataset_train = dataset_train.batch(BATCH_SIZE)\n",
        "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_val = dataset_val.batch(BATCH_SIZE)\n",
        "dataset_val = dataset_val.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_test = dataset_test.batch(BATCH_SIZE)\n",
        "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c6b699",
      "metadata": {
        "id": "15c6b699"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "eef43578",
      "metadata": {
        "id": "eef43578"
      },
      "outputs": [],
      "source": [
        "#트랜스포머 모델 객체 선언\n",
        "transformer = Transformer(\n",
        "    num_layers=3,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    dff=1024,\n",
        "    input_vocab_size=VOCAB_SIZE,\n",
        "    target_vocab_size=VOCAB_SIZE,\n",
        "    pe_input=MAX_LENGTH, # 포지셔닝 인코딩의 입력문장의 최대길이\n",
        "    pe_target=MAX_LENGTH, # # 그럼 이건 포지셔닝 인코딩의 출력문장의 최대길이겠지?\n",
        "    rate=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aa4ebe67",
      "metadata": {
        "id": "aa4ebe67"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000, multiplier=1.0):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.multiplier = multiplier\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # 학습률 계산 공식\n",
        "\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return self.multiplier * tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        # 직렬화를 위한 설정 반환\n",
        "        return {\n",
        "            \"d_model\": self.d_model,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "            \"multiplier\": self.multiplier\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    # 패딩 마스크\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask  # 패딩 제외한 토큰만 계산\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "#learning_rate = CustomSchedule(128)  # 네 모델에서 d_model 사용했을 거야\n",
        "learning_rate = CustomSchedule(d_model=128, warmup_steps=4000, multiplier=1.2)\n",
        "optimizer = tf.keras.optimizers.AdamW(weight_decay=1e-4 ,learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    real = tf.cast(real, tf.int64)\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eec91b12",
      "metadata": {
        "id": "eec91b12"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "\n",
        "## 모델체크포인트 콜백 만들기\n",
        "#checkpoint = ModelCheckpoint(\n",
        "#    'my_chatbot.keras',        # 저장할 파일 이름\n",
        "#    save_best_only=True,  # val_loss가 가장 좋을 때만 저장\n",
        "#    monitor='val_loss',   # 기준은 검증 손실\n",
        "#)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(dataset_train,  validation_data=dataset_val, epochs=100, callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "hXrIA4Ny6VRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da806269-b81e-4bbb-e488-94b409328f5f"
      },
      "id": "hXrIA4Ny6VRj",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 57ms/step - accuracy_function: 0.0789 - loss: 1.3740 - val_accuracy_function: 0.1337 - val_loss: 1.1155\n",
            "Epoch 2/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 50ms/step - accuracy_function: 0.1374 - loss: 1.0981 - val_accuracy_function: 0.1549 - val_loss: 1.0425\n",
            "Epoch 3/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 51ms/step - accuracy_function: 0.1551 - loss: 1.0374 - val_accuracy_function: 0.1661 - val_loss: 1.0078\n",
            "Epoch 4/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 50ms/step - accuracy_function: 0.1640 - loss: 1.0016 - val_accuracy_function: 0.1716 - val_loss: 0.9871\n",
            "Epoch 5/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 50ms/step - accuracy_function: 0.1714 - loss: 0.9750 - val_accuracy_function: 0.1782 - val_loss: 0.9666\n",
            "Epoch 6/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 51ms/step - accuracy_function: 0.1779 - loss: 0.9558 - val_accuracy_function: 0.1817 - val_loss: 0.9543\n",
            "Epoch 7/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 50ms/step - accuracy_function: 0.1846 - loss: 0.9270 - val_accuracy_function: 0.1870 - val_loss: 0.9421\n",
            "Epoch 8/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 52ms/step - accuracy_function: 0.1922 - loss: 0.9025 - val_accuracy_function: 0.1887 - val_loss: 0.9354\n",
            "Epoch 9/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 50ms/step - accuracy_function: 0.1967 - loss: 0.8812 - val_accuracy_function: 0.1918 - val_loss: 0.9333\n",
            "Epoch 10/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 50ms/step - accuracy_function: 0.1998 - loss: 0.8648 - val_accuracy_function: 0.1913 - val_loss: 0.9304\n",
            "Epoch 11/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 50ms/step - accuracy_function: 0.2023 - loss: 0.8512 - val_accuracy_function: 0.1924 - val_loss: 0.9310\n",
            "Epoch 12/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 51ms/step - accuracy_function: 0.2048 - loss: 0.8371 - val_accuracy_function: 0.1919 - val_loss: 0.9307\n",
            "Epoch 13/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 50ms/step - accuracy_function: 0.2060 - loss: 0.8291 - val_accuracy_function: 0.1929 - val_loss: 0.9311\n",
            "Epoch 14/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 50ms/step - accuracy_function: 0.2078 - loss: 0.8186 - val_accuracy_function: 0.1938 - val_loss: 0.9327\n",
            "Epoch 15/100\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 50ms/step - accuracy_function: 0.2112 - loss: 0.8087 - val_accuracy_function: 0.1914 - val_loss: 0.9346\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7851e98918d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRXoCoUcCli5"
      },
      "id": "fRXoCoUcCli5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3abd79f1",
      "metadata": {
        "id": "3abd79f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171ee8e4-3426-4e2a-94f3-a9f636d19d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy_function: 0.1947 - loss: 0.9164\n",
            "테스트 손실: 0.9159\n",
            "테스트 정확도: 0.1930\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = transformer.evaluate(dataset_test)\n",
        "print(f\"테스트 손실: {test_loss:.4f}\")\n",
        "print(f\"테스트 정확도: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aP13-96wcLnW"
      },
      "id": "aP13-96wcLnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXZAjpQecLVK"
      },
      "id": "tXZAjpQecLVK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39902a2f",
      "metadata": {
        "id": "39902a2f"
      },
      "outputs": [],
      "source": [
        "#def preprocess_sentence(sentence):\n",
        "#  # 단어와 구두점 사이에 공백 추가.\n",
        "#  # ex) 12시 땡! -> 12시 땡 !\n",
        "#  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "#  sentence = sentence.strip()\n",
        "#  return sentence\n",
        "\n",
        "#def evaluate(sentence):\n",
        "#  # 입력 문장에 대한 전처리\n",
        "#  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "#  # 입력 문장에 시작 토큰과 종료 토큰을 추가\n",
        "#  sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)  #인코더 인풋\n",
        "\n",
        "#  output = tf.expand_dims(START_TOKEN, 0)  #디코더 인풋\n",
        "\n",
        "#      # 디코더의 예측 시작\n",
        "#  for i in range(MAX_LENGTH):\n",
        "#    predictions = transformer(inputs=[sentence, output], training=False) # training=False: 트랜스포머 예측만 수행\n",
        "\n",
        "#    # 현재 시점의 예측 단어를 받아온다.\n",
        "#    predictions = predictions[:, -1:, :]  #[1, 1, vocab_size] ==> 마지막 토큰에 대한 확률을 모든 단어에 대해서 가져온다.\n",
        "\n",
        "#    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) #tf.argmax는 확률이 가장 높은 단어 토큰의 인덱스를 빼온다.\n",
        "                                                # (axis = -1) ==> vocab_size의 차원에서 가져오겠다는 뜻. 단어의 확률분포는 거기에 있으니까.\n",
        "\n",
        "#    # 만약 현재 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
        "#    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "#      break\n",
        "\n",
        "#    # 현재 시점의 예측 단어를 output(출력)에 연결한다.\n",
        "#    # output은 for문의 다음 루프에서 디코더의 입력이 된다.\n",
        "#    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "#  # 단어 예측이 모두 끝났다면 output을 리턴. ==> 한 문장의 토큰들의 배열을 리스트로 리턴.\n",
        "#  return tf.squeeze(output, axis=0)    ## 배치차원을 없앤다. 왜냐고? 한문장이니깐.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRlcT7NScLlC"
      },
      "id": "VRlcT7NScLlC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wz-gfKbqcLee"
      },
      "id": "Wz-gfKbqcLee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  # 단어와 구두점 사이에 공백 추가.\n",
        "  # ex) 12시 땡! -> 12시 땡 !\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_sampled(sentence, top_k=10):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "    output = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        predictions = transformer([sentence, output], training=False)\n",
        "        predictions = predictions[:, -1:, :]  # [1, 1, vocab_size]\n",
        "        predictions = tf.squeeze(predictions, axis=0).numpy()[0]\n",
        "\n",
        "        # 상위 top_k 개 중 무작위 선택\n",
        "        top_k_indices = predictions.argsort()[-top_k:]\n",
        "        top_k_probs = tf.nn.softmax(predictions[top_k_indices]).numpy()\n",
        "        sampled_index = np.random.choice(top_k_indices, p=top_k_probs)\n",
        "\n",
        "        predicted_id = tf.constant([[sampled_index]])\n",
        "\n",
        "        if tf.equal(predicted_id[0][0], END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "metadata": {
        "id": "BSvr7T1awXJ0"
      },
      "id": "BSvr7T1awXJ0",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "efe1237f",
      "metadata": {
        "id": "efe1237f"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "  real_predict = evaluate_sampled(sentence)\n",
        "\n",
        "  # real_predict == 디코더가 리턴한 챗봇의 대답에 해당하는 정수 시퀀스\n",
        "  # tokenizer.decode()를 통해 정수 시퀀스를 문자열로 디코딩.\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in real_predict if i < tokenizer.vocab_size])\n",
        "  # i < tokenizer.vocab_size 로 필터링 해주는 이유는\n",
        "  #**특수 토큰(UNK, PAD, START, END 등)**이 tokenizer의 vocab 범위 바깥에 있을 수도 있기 때문\n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Output: {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = predict(\"오잉??\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geBzTcYUwGHG",
        "outputId": "6109519e-01f6-4f63-eaca-0adf96dd0b4a"
      },
      "id": "geBzTcYUwGHG",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 오잉??\n",
            "Output: 응 근데 내가 보기엔 안됨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = predict(\"아니 진짜 뭐하는거야 정말\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fKaeAktfyqY",
        "outputId": "d10dc58d-887b-477c-cab6-335075dfa806"
      },
      "id": "-fKaeAktfyqY",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 아니 진짜 뭐하는거야 정말\n",
            "Output: 응 맞아 그래서 그거 엄청 비싸\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = predict(\"정말 기분 좋은 하루야\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaADEXdef5o9",
        "outputId": "186bdfb1-c50d-4c00-c457-6c375d768186"
      },
      "id": "vaADEXdef5o9",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 정말 기분 좋은 하루야\n",
            "Output: 그래도 그렇게 생각하면 좋겠다 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = predict(\"이 모델의 정확도가 너무 낮아서 나는 굉장히 화가 나\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yhAghovf_u5",
        "outputId": "3d777ab4-a54c-49ac-994f-e297264a894e"
      },
      "id": "9yhAghovf_u5",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 이 모델의 정확도가 너무 낮아서 나는 굉장히 화가 나\n",
            "Output: 나도 진짜 잘 몰라\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어떤 짓을 해도 정확도와 손실이 개선이 안된다... 아마 모델 자체에 문제가 있거나 토크나이저를 다른 것을 써봐야 할 것 같다....."
      ],
      "metadata": {
        "id": "HmxPdlsYuCAw"
      },
      "id": "HmxPdlsYuCAw"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}