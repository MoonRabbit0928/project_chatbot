{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11f3463b-6204-4dc7-97e1-26c993c44c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = pos / tf.pow(10000, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        return angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            pos=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        return pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(q, k, v, mask):              #(Q, K, VëŠ” ë‹¤ 4D í…ì„œì•¼: (batch_size, num_heads, seq_len, depth))\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(layers.Layer):        #num_head = í—¤ë“œê°œìˆ˜ <ì˜ˆ)8ê°œ>\n",
    "    def __init__(self, d_model, num_heads):   #d_model: ì „ì²´ ì¶œë ¥ ì°¨ì› (ì˜ˆ: 512)\n",
    "        super().__init__()                     #ì „ì²´ ì°¨ì›ì´ head ìˆ˜ë¡œ ì •í™•íˆ ë‚˜ëˆ ë–¨ì–´ì ¸ì•¼ í•´.\n",
    "        assert d_model % num_heads == 0         #ì˜ˆë¥¼ ë“¤ë©´ d_model=512, num_heads=8ì´ë©´ headë‹¹ depthëŠ” 512/8=64ì•¼.\n",
    "\n",
    "        self.num_heads = num_heads           # í—¤ë“œ ê°œìˆ˜ ì €ì¥\n",
    "        self.depth = d_model // num_heads     # ê¹Šì´ ì €ì¥\n",
    "\n",
    "        self.wq = layers.Dense(d_model)      # Query, Key, Valueë¥¼ ê°ê° Dense ë ˆì´ì–´ë¡œ ë³€í™˜í•  ì¤€ë¹„\n",
    "        self.wk = layers.Dense(d_model)      #ê°ê° ì…ë ¥ì„ d_model ì°¨ì›ìœ¼ë¡œ ë³€í™˜ì‹œì¼œ.\n",
    "        self.wv = layers.Dense(d_model)      #ì—¬ëŸ¬ headë¥¼ ë§Œë“¤ë ¤ë©´ Q, K, Vë¥¼ í•œë²ˆ linear transform í•´ì¤˜ì•¼ í•¨. (ë…ë¦½ì ìœ¼ë¡œ headë“¤ì´ ì¼í•  ìˆ˜ ìˆë„ë¡)\n",
    "\n",
    "        self.dense = layers.Dense(d_model)   #ë§ˆì§€ë§‰ì— ì—¬ëŸ¬ headë¥¼ ì´ì–´ë¶™ì¸ ê±¸ ë‹¤ì‹œ d_model í¬ê¸°ë¡œ ë§ì¶°ì£¼ëŠ” Dense ë ˆì´ì–´.\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        ## 1. (batch_size, seq_len, d_model) â†’ (batch_size, seq_len, num_heads, depth)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  #xë¥¼ (batch_size, seq_len, num_heads, depth)ë¡œ reshape.\n",
    "        # -1ì€ seq_len ê°™ì€ ì• ë“¤ì´ ê³ ì •ì´ ì•„ë‹ ë•Œ ìë™ìœ¼ë¡œ ë§ì¶°ì£¼ë ¤ê³  ì“°ëŠ” ê±°ì•¼!\n",
    "        \n",
    "        # 2. (batch_size, seq_len, num_heads, depth) â†’ (batch_size, num_heads, seq_len, depth)\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])                        \n",
    "                                                                         #ê·¸ë¦¬ê³  num_heads ì°¨ì›ì´ ì•ìœ¼ë¡œ ì˜¤ê²Œ transpose.\n",
    "                                                                         #ìµœì¢… shape: (batch_size, num_heads, seq_len, depth)\n",
    "                                                                         #ë©€í‹°í—¤ë“œë‹ˆê¹Œ, headë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ attentionì„ ê³„ì‚°í•˜ë ¤ê³  ì´ë ‡ê²Œ ë‚˜ëˆ”.\n",
    "                                                                                 \n",
    "    \n",
    "    def call(self, v, k, q, attention_mask=None, training=False):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)   #ì…ë ¥ q, k, vì— ëŒ€í•´ Dense í†µê³¼ì‹œì¼œì¤Œ.\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  #ê·¸ë¦¬ê³  ê°ê°ì„ ë©€í‹°í—¤ë“œ í˜•íƒœë¡œ ìª¼ê°¬.\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, attention_mask)            #ê°ê°ì˜ headì— ëŒ€í•´ Scaled Dot-Product Attention ì ìš©.\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])         #q, k, vê°€ ë©€í‹°í—¤ë“œ ëª¨ì–‘ì´ë¼ ë³‘ë ¬ë¡œ ì ìš©ë¨.\n",
    "    \n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
    "        #headë“¤ì„ í•˜ë‚˜ë¡œ ë¶™ì„ (concat)\n",
    "        #shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  #ë§ˆì§€ë§‰ Denseë¥¼ ê±°ì³ì„œ ìµœì¢… output ë§Œë“¦.\n",
    "        return output\n",
    "       #[ì…ë ¥ Q, K, V] \n",
    "       #  -> (ê°ê° Denseì— í†µê³¼) \n",
    "       #  -> (split_headsë¡œ ì—¬ëŸ¬ headë¡œ ë‚˜ëˆ”)\n",
    "       #  -> (ê° headë§ˆë‹¤ scaled dot-product attention ê³„ì‚°)\n",
    "       #  -> (headë“¤ concat)\n",
    "       #  -> (ë§ˆì§€ë§‰ Denseë¡œ ì¶œë ¥)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------- ì¸ì½”ë”ì™€ ë””ì½”ë” êµ¬ì¡° ------------------------------------#\n",
    "\n",
    "#ğŸ“˜ Encoder Layer\n",
    "#[Input] \n",
    "# â””â–¶ MultiHeadAttention(Self)\n",
    "#â””â–¶ Add & Norm\n",
    "# â””â–¶ Feed Forward\n",
    "# â””â–¶ Add & Norm\n",
    "# â””â–¶ Output to next encoder block\n",
    "\n",
    "#ğŸ“— Decoder Layer\n",
    "#[Previous Output + Encoder Output]\n",
    "# â””â–¶ MultiHeadAttention(Self + Mask)\n",
    "# â””â–¶ Add & Norm\n",
    "# â””â–¶ MultiHeadAttention(from Encoder Output)\n",
    "# â””â–¶ Add & Norm\n",
    "# â””â–¶ Feed Forward\n",
    "# â””â–¶ Add & Norm\n",
    "# â””â–¶ Output to next decoder block\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(layers.Layer):                             ## <dffì™€ rateëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°!!>\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):    ## d_model: ì„ë² ë”© ì°¨ì›. ë³´í†µ 128, 256, 512, 768 ê°™ì€ 2^n ê°’.\n",
    "        super().__init__()                                    ## dff: FFN ë‚´ë¶€ ì€ë‹‰ì¸µì˜ ì°¨ì›. ë³´í†µ d_model * 2 í˜¹ì€ d_model * 4 ì •ë„.\n",
    "                                                              ##  rate: ë“œë¡­ì•„ì›ƒ í™•ë¥ . 0.1ì´ë©´ 10%ë¥¼ ëœë¤ìœ¼ë¡œ ëŠì–´.\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  \n",
    "\n",
    "        self.ffn = tf.keras.Sequential([            ### Feed Forward MLP ì€ë‹‰ì¸µ í•œê°œ, ì¶œë ¥ì¸µ í•œê°œ. ==> ê°€ì¤‘ì¹˜ ì¤˜ì„œ ì°¨ì› ë†’ì˜€ë‹¤ê°€ ì¤„ì„. \n",
    "            layers.Dense(dff, activation='relu'),   ### ì°¨ì›ì„ ë†’ì˜€ë‹¤ê°€ ì¤„ì´ëŠ” ì´ìœ ëŠ” ì •ë³´ í‘œí˜„ ëŠ¥ë ¥ì„ ë†’ì´ê¸° ìœ„í•¨ì´ë‹¤.\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        ## ì¸µ ì •ê·œí™”                                                  #epsilon=1e-6: ë¶„ëª¨ê°€ 0 ë˜ëŠ” ê±¸ ë°©ì§€í•˜ëŠ” ì•„ì£¼ ì‘ì€ ê°’.\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)    #ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ í”¼ì²˜ ë‹¨ìœ„ë¡œ ì •ê·œí™”í•œë‹¤.\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)    #ê° ìœ„ì¹˜ì˜ ì„ë² ë”© ë²¡í„° (ì˜ˆ: [0.2, -0.4, 1.3, ...]) ë¥¼ í‰ê·  0, ë¶„ì‚° 1ë¡œ ì •ê·œí™”.\n",
    "\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)                         #ë“œë¡­ì•„ì›ƒ ì¸µ ë‘ê°œ \n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None, training=False):                                    \n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)                               #1.Self-Attention ìˆ˜í–‰\n",
    "        attn_output = self.dropout1(attn_output, training=training)         #2.ë“œë¡­ì•„ì›ƒ ==> í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ëŠ” ì€ë‹‰ì¸µì²˜ëŸ¼ í™œìš©\n",
    "        out1 = self.layernorm1(x + attn_output)                             #3.ì”ì°¨ì—°ê²° ë° ì •ê·œí™” <ì—¬ê¸°ì„œ ì”ì°¨ì—°ê²°ì´ë€ input + attn_outputì¸ë° >\n",
    " #<ì—¬ê¸°ì„œ ì”ì°¨ì—°ê²°ì´ë€ input + attn_outputì¸ë° ì •ë³´ì†ì‹¤ ë°©ì§€ ì—­í• ì„ í•œë‹¤ ê·¸ëƒ¥ ì™¸ì›Œ>\n",
    "        \n",
    "        ffn_output = self.ffn(out1)                                         #4.FFN í†µê³¼ ==> ë“œë¡­ì•„ì›ƒì„ ê±°ì¹œ ë‰´ëŸ°ë“¤ì´ FFNì„ í†µê³¼í•˜ë©´ì„œ ë‹¤ì‹œ ë¹„ì¤‘ì´ ìƒê¹€\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)           #5.ë‹¤ì‹œ ë“œë¡­ì•„ì›ƒ => ë‹¤ì‹œ í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ëŠ” ì€ë‹‰ì¸µ ì²˜ëŸ¼ í™œìš©\n",
    "        out2 = self.layernorm2(out1 + ffn_output)                           #6.ì”ì°¨ì—°ê²° ë° ì •ê·œí™” ==>ë\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n",
    "        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)                      #1ë‹¨ê³„: Masked Self-Attention\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask)    #2ë‹¨ê³„: Encoder-Decoder Attention\n",
    "        attn2 = self.dropout2(attn2, training=training)                  #Query: ë””ì½”ë”ê°€ í˜„ì¬ê¹Œì§€ ë§Œë“  ì¶œë ¥ (ë””ì½”ë”ì˜ ìƒíƒœ)\n",
    "        out2 = self.layernorm2(attn2 + out1)                             #Key, Value: ì¸ì½”ë”ì˜ ì¶œë ¥ (ì „ì²´ ë¬¸ì¥ì˜ ì˜ë¯¸ ë²¡í„°ë“¤)\n",
    "                                                                         #ì´ê±¸ë¡œ ì–´í…ì…˜ â†’ context vectorë¥¼ êµ¬í•¨\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3\n",
    "\n",
    "\n",
    "#<ë””ì½”ë” ë ˆì´ì–´ ì „ì²´ ê·¸ë¦¼>\n",
    "\n",
    "#Input x\n",
    "#  â†“\n",
    "#Masked MultiHead Attention (Q=K=V=x, look-ahead mask)\n",
    "#  â†“\n",
    "#Residual + LayerNorm\n",
    "#  â†“\n",
    "#MultiHead Attention with Encoder Output (Q=Decoder, K=V=Encoder)\n",
    "#  â†“\n",
    "#Residual + LayerNorm\n",
    "#  â†“\n",
    "#Feed Forward Network (Dense â†’ ReLU â†’ Dense)\n",
    "#  â†“\n",
    "#Residual + LayerNorm\n",
    "#  â†“\n",
    "#Output (xì˜ ë””ì½”ë” ì²˜ë¦¬ ê²°ê³¼)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#í•­ëª©\t     Encoderì˜ Multi-Head Attention\t          vs               Decoderì˜ Masked Multi-Head Self-Attention\n",
    "\n",
    "#ëª©ì \t     ì…ë ¥ ì „ì²´ë¥¼ ìŠ¤ìŠ¤ë¡œ ì´í•´ (ê¸€ì˜ ì˜ë¯¸ íŒŒì•…)\t                   ì´ì „ ë‹¨ì–´ë§Œ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡\n",
    "#ë§ˆìŠ¤í¬\t     Padding maskë§Œ ì‚¬ìš©\t                                       Padding mask + Look-ahead mask ì‚¬ìš©\n",
    "#ì •ë³´ ì ‘ê·¼\t ëª¨ë“  í† í°ì— ììœ ë¡­ê²Œ ì ‘ê·¼\t                               ìê¸° ì•ì˜ ë‹¨ì–´ê¹Œì§€ë§Œ ì ‘ê·¼ ê°€ëŠ¥ (ë¯¸ë˜ ì°¨ë‹¨)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Encoder\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                                                ##maximum_position_encoding : ì…ë ¥ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì˜ë¯¸í•´!\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers     ## ì¸ì½”ë”© ì¸µì„ ì–¼ë§ˆë‚˜ í†µê³¼ì‹œí‚¬ ê±´ì§€ => ë³´í†µ 6ê°œë¥¼ ë§ì´ ì“´ë‹¤ê³  í•œë‹¤.\n",
    "\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))   \n",
    "        #ì´ê±´ ì„ë² ë”©ëœ ë²¡í„°ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •í•´ì£¼ëŠ” ì½”ë“œì•¼. ë©€í‹°í—¤ë“œì–´í…ì…˜ê³¼ ë‹¤ë¥¸ ì ì€ ìŠ¤ì¼€ì¼ì„ í‚¤ì›Œì¤€ë‹¤!!\n",
    "        \n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for enc_layer in self.enc_layers:           ##\n",
    "            x = enc_layer(x, mask=mask, training=training)\n",
    "\n",
    "        return x        #ìµœì¢… ì¶œë ¥ xëŠ” í¬ê¸°ê°€ (batch_size, input_seq_len, d_model)ì¸ í…ì„œ\n",
    "                        #ì´ê±¸ ë””ì½”ë”ì—ì„œ í‚¤-ê°’ ë²¡í„°ë¡œ ì°¸ì¡°í•´ì„œ ì˜ˆì¸¡ì— ì‚¬ìš©í•´ ==>> ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜ì—ì„œì˜ KEY, VALUEë¡œ inputë˜ëŠ” ê°’ì´ ëœë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 100ì ì´ë‚´ì˜ ì§§ì€ ë¬¸ì¥ë§Œ ë‹¤ë£¬ë‹¤ë©´\n",
    "#maximum_position_encoding = 100\n",
    "\n",
    "# ë¬¸ì¥ ë²ˆì—­ìš© ì¼ë°˜ Transformerë¼ë©´\n",
    "#maximum_position_encoding = 512\n",
    "\n",
    "# ë…¼ë¬¸ ì „ì²´ì²˜ëŸ¼ ê¸´ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ê±°ë¼ë©´\n",
    "#maximum_position_encoding = 1024 ~ 4096\n",
    "\n",
    "# Decoder\n",
    "class Decoder(layers.Layer):            #target_vocab_size : ì˜ˆì¸¡í•  ëŒ€ìƒ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°  ///  maximum_position_encoding\t: ìœ„ì¹˜ ì¸ì½”ë”©ì—ì„œ ìµœëŒ€ ë¬¸ì¥ê¸¸ì´\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                 #dff => FFN ì€ë‹‰ì¸µ ì°¨ì› \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)   \n",
    "        #ë””ì½”ë”ì˜ embedding()ì€ **\"ì •ë‹µ ë¬¸ì¥\"ì˜ ì•ë¶€ë¶„(ì˜ˆì¸¡í•´ì•¼ í•  ë¬¸ì¥ì˜ ì´ì „ ë‹¨ì–´ë“¤)**ì„ ì„ë² ë”©í•˜ëŠ” ê±°ì•¼.\n",
    "\n",
    "#ë‹¨ê³„\t                     ì…ë ¥ê°’\t                        ì„¤ëª…\n",
    "#Encoder input\t             \"ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤\"\t            ì¸ì½”ë”ì— ë“¤ì–´ê°ˆ ì…ë ¥ ë¬¸ì¥ (source)\n",
    "#Decoder input\t             [\"<s>\", \"I\", \"ate\"]\t        ë””ì½”ë”ì— ë“¤ì–´ê°ˆ ì…ë ¥ (targetì˜ ì•ë¶€ë¶„)\n",
    "#Decoder output (target)\t [\"I\", \"ate\", \"rice\"]\t        ë””ì½”ë”ê°€ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ì‹¤ì œ ì •ë‹µ\n",
    "       \n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)   \n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for dec_layer in self.dec_layers:\n",
    "            x = dec_layer(x, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask, training=training)\n",
    "\n",
    "        return x   #(batch_size, target_seq_len, target_vocab_size)   #ë””ì½”ë” ê° ìœ„ì¹˜ì—ì„œ ìµœì¢…ì ìœ¼ë¡œ ë§Œë“¤ì–´ë‚¸ ë²¡í„°ë“¤\n",
    "\n",
    "\n",
    "\n",
    "# Transformer\n",
    "\n",
    "class Transformer(tf.keras.Model):   \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()   \n",
    "        #input_vocab_size:ì¸ì½”ë”ì— ë“¤ì–´ê°ˆ ë¬¸ì¥ì˜ ë‹¨ì–´ ì§‘í•© í¬ê¸° (vocabulary size)\n",
    "        #target_vocab_size : ë””ì½”ë”ê°€ ìƒì„±í•  ë¬¸ì¥ì˜ ë‹¨ì–´ ì§‘í•© í¬ê¸°\n",
    "        #pe_input : Positional Encodingì„ ìœ„í•œ ì…ë ¥ ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´, ì˜ˆ: ì…ë ¥ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ê°€ 100ì´ë¼ë©´ â†’ pe_input = 100\n",
    "        #pe_target : ë””ì½”ë” ì…ë ¥ (ì •ë‹µ ë¬¸ì¥)ì˜ ìµœëŒ€ ê¸¸ì´, ì˜ˆ: : ì •ë‹µ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ê°€ 80ì´ë¼ë©´ â†’ pe_target = 80\n",
    "        #ì£¼ì˜: pe_input, pe_targetì€ í•™ìŠµ ë°ì´í„°ë³´ë‹¤ ì¶©ë¶„íˆ í¬ê²Œ ì¡ì! ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ë³´ë‹¤ ì‚´ì§ í¬ê²Œ ì¡ì•„ë‘ëŠ” ê²Œ ì•ˆì „í•¨ (ex. 128, 256 ë“±)\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # unpacking\n",
    "        inp, tar = inputs\n",
    "\n",
    "        # ë§ˆìŠ¤í¬ ìƒì„±\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "\n",
    "        # ì¸ì½”ë”-ë””ì½”ë” íë¦„\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask, training=training)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output\n",
    "        #ë¡œì§“ (logits): ì•„ì§ softmaxëŠ” ì•ˆ ì”Œìš´ ìƒ í™•ë¥  ì ìˆ˜ë“¤. \n",
    "        #ë³´í†µ loss ê³„ì‚°í•  ë•Œ softmax ì—†ì´ logitsì„ ì“°ëŠ” ê²Œ ë” ì•ˆì •ì ì´ê¸° ë•Œë¬¸ì— ì´ë ‡ê²Œ ë°˜í™˜í•´.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#Input Sentence  --->  Encoder ----------------------------â”\n",
    "#                    â†“                                     â†“\n",
    "#               Encoded Context  -->  Decoder (with target input)\n",
    "#                                          â†“\n",
    "#                                 Final Dense (Softmax logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\"ì…ë ¥ í† í° ID\" = ì •ìˆ˜ ì¸ì½”ë”©ëœ ê°’\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ìš”ì•½ íë¦„\n",
    "#ì…ë ¥ë¬¸ì¥ inp â†’ ì¸ì½”ë”ì— ë“¤ì–´ê° â†’ enc_output ìƒì„±\n",
    "\n",
    "#ì •ë‹µ ë¬¸ì¥ tar â†’ ë””ì½”ë”ì— ë“¤ì–´ê° (enc_output ì°¸ê³ í•¨)\n",
    "\n",
    "#ë””ì½”ë” ì¶œë ¥ dec_output â†’ Dense layer â†’ ë‹¨ì–´ í™•ë¥  ë¶„í¬ final_output\n",
    "\n",
    "#ë¦¬í„´: (batch, target_seq_len, vocab_size) í˜•íƒœì˜ ë¡œì§“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cd1620-748f-48b0-9edc-62586223ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = pos / tf.pow(10000, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        return angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            pos=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        return pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):              \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):       \n",
    "    def __init__(self, d_model, num_heads):   \n",
    "        super().__init__()                     \n",
    "        assert d_model % num_heads == 0        \n",
    "\n",
    "        self.num_heads = num_heads           \n",
    "        self.depth = d_model // num_heads   \n",
    "\n",
    "        self.wq = layers.Dense(d_model)     \n",
    "        self.wk = layers.Dense(d_model)      \n",
    "        self.wv = layers.Dense(d_model)     \n",
    "\n",
    "        self.dense = layers.Dense(d_model)   \n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, v, k, q, attention_mask=None, training=False):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)   #ì…ë ¥ q, k, vì— ëŒ€í•´ Dense í†µê³¼ì‹œì¼œì¤Œ.\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  #ê·¸ë¦¬ê³  ê°ê°ì„ ë©€í‹°í—¤ë“œ í˜•íƒœë¡œ ìª¼ê°¬.\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, attention_mask)            \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])         \n",
    "    \n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
    "        output = self.dense(concat_attention)  \n",
    "        return output\n",
    "\n",
    "class EncoderLayer(layers.Layer):                           \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):    \n",
    "        super().__init__()                                    \n",
    "                                                              \n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  \n",
    "\n",
    "        self.ffn = tf.keras.Sequential([          \n",
    "            layers.Dense(dff, activation='relu'),   \n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        ## ì¸µ ì •ê·œí™”                                                  \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)    \n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)    \n",
    "\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)                         \n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None, training=False):                                    \n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)                               \n",
    "        attn_output = self.dropout1(attn_output, training=training)        \n",
    "        out1 = self.layernorm1(x + attn_output)                             \n",
    "        \n",
    "        ffn_output = self.ffn(out1)                                         \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)           \n",
    "        out2 = self.layernorm2(out1 + ffn_output)                           \n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n",
    "        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)                 \n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask)    \n",
    "        attn2 = self.dropout2(attn2, training=training)                  \n",
    "        out2 = self.layernorm2(attn2 + out1)                             \n",
    "                                                                         \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                                             \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers \n",
    "\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))   \n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for enc_layer in self.enc_layers:           \n",
    "            x = enc_layer(x, mask=mask, training=training)\n",
    "\n",
    "        return x  \n",
    "\n",
    "class Decoder(layers.Layer):            \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                 \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)        \n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)   \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for dec_layer in self.dec_layers:\n",
    "            x = dec_layer(x, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(tf.keras.Model):   \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()   \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        inp, tar = inputs\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask, training=training)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d92387f-465d-4131-9f6a-c902ddd0137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = pos / tf.pow(10000, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        return angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            pos=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        return pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):              \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):       \n",
    "    def __init__(self, d_model, num_heads):   \n",
    "        super().__init__()                     \n",
    "        assert d_model % num_heads == 0        \n",
    "\n",
    "        self.num_heads = num_heads           \n",
    "        self.depth = d_model // num_heads   \n",
    "\n",
    "        self.wq = layers.Dense(d_model)     \n",
    "        self.wk = layers.Dense(d_model)      \n",
    "        self.wv = layers.Dense(d_model)     \n",
    "\n",
    "        self.dense = layers.Dense(d_model)   \n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, v, k, q, attention_mask=None, training=False):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)   #ì…ë ¥ q, k, vì— ëŒ€í•´ Dense í†µê³¼ì‹œì¼œì¤Œ.\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  #ê·¸ë¦¬ê³  ê°ê°ì„ ë©€í‹°í—¤ë“œ í˜•íƒœë¡œ ìª¼ê°¬.\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, attention_mask)            \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])         \n",
    "    \n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
    "        output = self.dense(concat_attention)  \n",
    "        return output\n",
    "\n",
    "class EncoderLayer(layers.Layer):                           \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):    \n",
    "        super().__init__()                                    \n",
    "                                                              \n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  \n",
    "\n",
    "        self.ffn = tf.keras.Sequential([          \n",
    "            layers.Dense(dff, activation='relu'),   \n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        ## ì¸µ ì •ê·œí™”                                                  \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)    \n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)    \n",
    "\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)                         \n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None, training=False):                                    \n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)                               \n",
    "        attn_output = self.dropout1(attn_output, training=training)        \n",
    "        out1 = self.layernorm1(x + attn_output)                             \n",
    "        \n",
    "        ffn_output = self.ffn(out1)                                         \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)           \n",
    "        out2 = self.layernorm2(out1 + ffn_output)                           \n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n",
    "        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)                 \n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask)    \n",
    "        attn2 = self.dropout2(attn2, training=training)                  \n",
    "        out2 = self.layernorm2(attn2 + out1)                             \n",
    "                                                                         \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                                             \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers \n",
    "\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))   \n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for enc_layer in self.enc_layers:           \n",
    "            x = enc_layer(x, mask=mask, training=training)\n",
    "\n",
    "        return x  \n",
    "\n",
    "class Decoder(layers.Layer):            \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                 \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)        \n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)   \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for dec_layer in self.dec_layers:\n",
    "            x = dec_layer(x, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(tf.keras.Model):   \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()   \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        inp, tar = inputs\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask, training=training)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debcf9c-95c3-42be-855d-15391aea2745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3ce3f-7668-487d-ba99-1777ef083a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7eb0d-1ddc-4186-8002-af827ac98ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90ea6d-f886-419a-a8b0-7bf326755219",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar ì— ê´€í•œ ê²ƒ\n",
    "ìƒí™©\t       ë””ì½”ë” ì…ë ¥\t                        ì˜ˆì¸¡ ë°©ì‹\t                Mask\n",
    "í›ˆë ¨\t    [\"<start>\", \"I\", \"love\", \"you\"]\t        ë³‘ë ¬ë¡œ ì „ì²´ ì˜ˆì¸¡\t            âœ… Look-ahead mask ì‚¬ìš©\n",
    "ì¶”ë¡ (ì˜ˆì¸¡)\t[\"<start>\"] â†’ [\"<start>\", \"I\"] â†’ ...\tìˆœì°¨ì ìœ¼ë¡œ í•œ í† í°ì”© ì˜ˆì¸¡\t    âœ… Look-ahead mask ì‚¬ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99803248-66ae-426e-b7ed-87c707dedb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f02e8a-0ae3-441d-9064-f68556fa11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ê¹Œì§€ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ë¯¸ë‹ˆíŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬í˜„ì´ ëë‚¬ë‹¤.\n",
    "# íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬í˜„ì„ í–ˆê³  ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ë…¸ë ¥í–ˆì§€ë§Œ, ì•„ë§ˆ ì™„ì „íˆ ì´í•´í•˜ì§€ëŠ” ëª»í–ˆì„ ê²ƒì´ë‹¤.\n",
    "# ì´ëŠ” ìˆ˜ë§ì€ ì—°ìŠµê³¼ ë‚˜ë§Œì˜ í”„ë¡œì íŠ¸ë¥¼ êµ¬í˜„í•˜ë©´ì„œ ê·¹ë³µí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.\n",
    "# í† ìŠ¤ ceoê°€ ì—´ì‹¬íˆ ë…¸ë ¥í•˜ë©´ ì–¸ì  ê°„ ê¸°íšŒëŠ” ì°¾ì•„ì˜¨ë‹¤ê³  í–ˆë‹¤. ë‚œ ê·¸ ê¸°íšŒë¥¼ ì¡ê¸° ìœ„í•´ì„œ ì—´ì‹¬íˆ ë…¸ë ¥í•˜ê³  ìˆëŠ” ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3395d-e9a0-4fbd-8542-6f9ef64e86f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8be6a9-5fbf-4480-9ddf-e6c8cd515eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì±—ë´‡\n",
    "\n",
    "âœ… ì±—ë´‡ êµ¬í˜„ íë¦„ (Transformer ê¸°ë°˜)\n",
    "1.ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "  ì§ˆë¬¸ê³¼ ë‹µë³€ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ëŒ€í™” ë°ì´í„°ì…‹ (ex. â€œì¡¸ì—… ìš”ê±´ì´ ë­ì•¼?â€ â†’ â€œì´ 130í•™ì ì´ í•„ìš”í•´ìš”â€)\n",
    "\n",
    "  ì •ìˆ˜ ì¸ì½”ë”© (tokenizer ì‚¬ìš©)\n",
    "\n",
    "2.ì…ì¶œë ¥ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "  íŒ¨ë”© / ë§ˆìŠ¤í‚¹ / ì‹œì‘ í† í° (<start>) / ì¢…ë£Œ í† í° (<end>) ë¶™ì´ê¸°\n",
    "\n",
    "  inp â†’ encoder ì…ë ¥\n",
    "\n",
    "  tar_input â†’ decoder ì…ë ¥\n",
    "\n",
    "  tar_real â†’ decoder ì¶œë ¥(ì •ë‹µ)\n",
    "\n",
    "3.íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ êµ¬í˜„\n",
    "\n",
    "  ë°©ê¸ˆ ë§Œë“  Transformer í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "\n",
    "  loss function, optimizer ì •ì˜\n",
    "\n",
    "4.ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "  teacher forcing ê¸°ë²• ì ìš©\n",
    "\n",
    "  ì—¬ëŸ¬ epoch ë™ì•ˆ í•™ìŠµ\n",
    "\n",
    "5.ì±—ë´‡ ì˜ˆì¸¡ (inference)\n",
    "\n",
    "  í•˜ë‚˜ì˜ ì§ˆë¬¸ ì…ë ¥ â†’ decoderê°€ í•œ ê¸€ìì”© ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ (auto-regressive)\n",
    "\n",
    "  <end> í† í°ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë°˜ë³µ\n",
    "\n",
    "6.CLI ê¸°ë°˜ ì±—ë´‡ ë§Œë“¤ê¸°\n",
    "\n",
    "  ì‚¬ìš©ì ì…ë ¥ ë°›ê¸° â†’ ëª¨ë¸ ì‘ë‹µ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f2a4a-6a50-4a7d-b7d3-bd2695a7e310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea023f76-feb7-4950-9db9-3a5ed11968e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87fb6328-17be-4e95-889d-31be1bfa6325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q_clean</th>\n",
       "      <th>A</th>\n",
       "      <th>A_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜</td>\n",
       "      <td>ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜</td>\n",
       "      <td>ì¡¸ì—…ìš”ê±´ì„ ì•Œë ¤ë©´ í•™ë²ˆê³¼ í•™ê³¼ë¥¼ ì•Œë ¤ì¤˜.</td>\n",
       "      <td>ì¡¸ì—…ìš”ê±´ì„ ì•Œë ¤ë©´ í•™ë²ˆê³¼ í•™ê³¼ë¥¼ ì•Œë ¤ì¤˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì–´í•™ ê¸°ì¤€ì´ ê¶ê¸ˆí•´</td>\n",
       "      <td>ì–´í•™ ê¸°ì¤€ì´ ê¶ê¸ˆí•´</td>\n",
       "      <td>ì–´í•™ ê¸°ì¤€ì€ í•™ë²ˆë³„ë¡œ ë‹¬ë¼. í•™ë²ˆì„ ì•Œë ¤ì¤„ë˜?</td>\n",
       "      <td>ì–´í•™ ê¸°ì¤€ì€ í•™ë²ˆë³„ë¡œ ë‹¬ë¼ í•™ë²ˆì„ ì•Œë ¤ì¤„ë˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë…¼ë¬¸ ê¼­ ì¨ì•¼ í•´?</td>\n",
       "      <td>ë…¼ë¬¸ ê¼­ ì¨ì•¼ í•´</td>\n",
       "      <td>ë…¼ë¬¸ ëŒ€ì‹  ì¡¸ì—… í”„ë¡œì íŠ¸ë‚˜ í¬íŠ¸í´ë¦¬ì˜¤ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆì–´.</td>\n",
       "      <td>ë…¼ë¬¸ ëŒ€ì‹  ì¡¸ì—… í”„ë¡œì íŠ¸ë‚˜ í¬íŠ¸í´ë¦¬ì˜¤ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆì–´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì´ ëª‡ í•™ì  ë“¤ì–´ì•¼ í•´?</td>\n",
       "      <td>ì´ ëª‡ í•™ì  ë“¤ì–´ì•¼ í•´</td>\n",
       "      <td>ì´ 130í•™ì  ì´ìƒ ì´ìˆ˜í•´ì•¼ ì¡¸ì—…í•  ìˆ˜ ìˆì–´.</td>\n",
       "      <td>ì´ 130í•™ì  ì´ìƒ ì´ìˆ˜í•´ì•¼ ì¡¸ì—…í•  ìˆ˜ ìˆì–´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>êµì–‘í•„ìˆ˜ ê³¼ëª© ì•Œë ¤ì¤˜</td>\n",
       "      <td>êµì–‘í•„ìˆ˜ ê³¼ëª© ì•Œë ¤ì¤˜</td>\n",
       "      <td>ëŒ€í•™ê³µí†µêµì–‘ìœ¼ë¡œëŠ” ê¸€ì“°ê¸°, ì˜ì–´, ì»´í“¨íŒ…ì‚¬ê³ ê°€ í¬í•¨ë¼.</td>\n",
       "      <td>ëŒ€í•™ê³µí†µêµì–‘ìœ¼ë¡œëŠ” ê¸€ì“°ê¸° ì˜ì–´ ì»´í“¨íŒ…ì‚¬ê³ ê°€ í¬í•¨ë¼</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Q       Q_clean                                A  \\\n",
       "0       ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜      ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜           ì¡¸ì—…ìš”ê±´ì„ ì•Œë ¤ë©´ í•™ë²ˆê³¼ í•™ê³¼ë¥¼ ì•Œë ¤ì¤˜.   \n",
       "1     ì–´í•™ ê¸°ì¤€ì´ ê¶ê¸ˆí•´    ì–´í•™ ê¸°ì¤€ì´ ê¶ê¸ˆí•´        ì–´í•™ ê¸°ì¤€ì€ í•™ë²ˆë³„ë¡œ ë‹¬ë¼. í•™ë²ˆì„ ì•Œë ¤ì¤„ë˜?   \n",
       "2     ë…¼ë¬¸ ê¼­ ì¨ì•¼ í•´?     ë…¼ë¬¸ ê¼­ ì¨ì•¼ í•´  ë…¼ë¬¸ ëŒ€ì‹  ì¡¸ì—… í”„ë¡œì íŠ¸ë‚˜ í¬íŠ¸í´ë¦¬ì˜¤ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆì–´.   \n",
       "3  ì´ ëª‡ í•™ì  ë“¤ì–´ì•¼ í•´?  ì´ ëª‡ í•™ì  ë“¤ì–´ì•¼ í•´        ì´ 130í•™ì  ì´ìƒ ì´ìˆ˜í•´ì•¼ ì¡¸ì—…í•  ìˆ˜ ìˆì–´.   \n",
       "4    êµì–‘í•„ìˆ˜ ê³¼ëª© ì•Œë ¤ì¤˜   êµì–‘í•„ìˆ˜ ê³¼ëª© ì•Œë ¤ì¤˜   ëŒ€í•™ê³µí†µêµì–‘ìœ¼ë¡œëŠ” ê¸€ì“°ê¸°, ì˜ì–´, ì»´í“¨íŒ…ì‚¬ê³ ê°€ í¬í•¨ë¼.   \n",
       "\n",
       "                          A_clean  \n",
       "0           ì¡¸ì—…ìš”ê±´ì„ ì•Œë ¤ë©´ í•™ë²ˆê³¼ í•™ê³¼ë¥¼ ì•Œë ¤ì¤˜  \n",
       "1         ì–´í•™ ê¸°ì¤€ì€ í•™ë²ˆë³„ë¡œ ë‹¬ë¼ í•™ë²ˆì„ ì•Œë ¤ì¤„ë˜  \n",
       "2  ë…¼ë¬¸ ëŒ€ì‹  ì¡¸ì—… í”„ë¡œì íŠ¸ë‚˜ í¬íŠ¸í´ë¦¬ì˜¤ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆì–´  \n",
       "3        ì´ 130í•™ì  ì´ìƒ ì´ìˆ˜í•´ì•¼ ì¡¸ì—…í•  ìˆ˜ ìˆì–´  \n",
       "4     ëŒ€í•™ê³µí†µêµì–‘ìœ¼ë¡œëŠ” ê¸€ì“°ê¸° ì˜ì–´ ì»´í“¨íŒ…ì‚¬ê³ ê°€ í¬í•¨ë¼  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í•´ë³´ì.\n",
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"gradu_qna_dummy.csv\", encoding = 'euc-kr')\n",
    "#print(df)\n",
    "\n",
    "# Drop rows with missing Q or A\n",
    "df.dropna(subset=[\"Q\", \"A\"], inplace=True)  # inplace=True  ==> ë³µì‚¬ë³¸ ë§Œë“¤ì§€ ë§ê³  ì›ë³¸ìœ¼ë¡œ\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"[^ê°€-í£0-9a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "df[\"Q_clean\"] = df[\"Q\"].apply(clean_text)\n",
    "df[\"A_clean\"] = df[\"A\"].apply(clean_text)\n",
    "\n",
    "# Show a sample\n",
    "df[[\"Q\", \"Q_clean\", \"A\", \"A_clean\"]].head()\n",
    "\n",
    "#questions = df['Q'].astype(str).tolist()\n",
    "#answers = df['A'].astype(str).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa39d62e-6666-4b14-ae07-c2b4f0140aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì¡¸ì—…ìš”ê±´ ì•Œë ¤ì¤˜', 'ì–´í•™ ê¸°ì¤€ì´ ê¶ê¸ˆí•´', 'ë…¼ë¬¸ ê¼­ ì¨ì•¼ í•´ ?', 'ì´ ëª‡ í•™ì  ë“¤ì–´ì•¼ í•´ ?', 'êµì–‘í•„ìˆ˜ ê³¼ëª© ì•Œë ¤ì¤˜']\n",
      "['ì¡¸ì—…ìš”ê±´ì„ ì•Œë ¤ë©´ í•™ë²ˆê³¼ í•™ê³¼ë¥¼ ì•Œë ¤ì¤˜ .', 'ì–´í•™ ê¸°ì¤€ì€ í•™ë²ˆë³„ë¡œ ë‹¬ë¼ .  í•™ë²ˆì„ ì•Œë ¤ì¤„ë˜ ?', 'ë…¼ë¬¸ ëŒ€ì‹  ì¡¸ì—… í”„ë¡œì íŠ¸ë‚˜ í¬íŠ¸í´ë¦¬ì˜¤ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆì–´ .', 'ì´ 130í•™ì  ì´ìƒ ì´ìˆ˜í•´ì•¼ ì¡¸ì—…í•  ìˆ˜ ìˆì–´ .', 'ëŒ€í•™ê³µí†µêµì–‘ìœ¼ë¡œëŠ” ê¸€ì“°ê¸° ,  ì˜ì–´ ,  ì»´í“¨íŒ…ì‚¬ê³ ê°€ í¬í•¨ë¼ .']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "train_data = pd.read_csv(\"gradu_qna_dummy.csv\", encoding = 'euc-kr')\n",
    "\n",
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    # êµ¬ë‘ì ì— ëŒ€í•´ì„œ ë„ì–´ì“°ê¸°\n",
    "    # ex) 12ì‹œ ë•¡! -> 12ì‹œ ë•¡ !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    # êµ¬ë‘ì ì— ëŒ€í•´ì„œ ë„ì–´ì“°ê¸°\n",
    "    # ex) 12ì‹œ ë•¡! -> 12ì‹œ ë•¡ !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)\n",
    "\n",
    "print(questions[:5])\n",
    "print(answers[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa19ef5-b1d9-47cd-a7dd-06acafda43f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‹œì‘ í† í° ë²ˆí˜¸ : [416]\n",
      "ì¢…ë£Œ í† í° ë²ˆí˜¸ : [417]\n",
      "ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : 418\n",
      "ë‚˜ëŠ” 2021í•™ë²ˆì´ê³  ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤í•™ë¶€ì•¼\n"
     ]
    }
   ],
   "source": [
    "# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸, ë‹µë³€ ë°ì´í„°ë¡œë¶€í„° ë‹¨ì–´ ì§‘í•©(Vocabulary) ìƒì„±\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "\n",
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ëŒ€í•œ ì •ìˆ˜ ë¶€ì—¬.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "#ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë§Œë“¤ì–´ë‘” ì´ìœ ëŠ” ë‚˜ì¤‘ì— input + START_TOKEN + END_TOKEN ê°™ì´ ì‰½ê²Œ ë¶™ì´ê¸° ìœ„í•´ì„œ.\n",
    "\n",
    "\n",
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "print('ì‹œì‘ í† í° ë²ˆí˜¸ :',START_TOKEN)\n",
    "print('ì¢…ë£Œ í† í° ë²ˆí˜¸ :',END_TOKEN)\n",
    "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :',VOCAB_SIZE)\n",
    "\n",
    "print(questions[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e529770-15a8-43ea-85a3-46e163c33e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ì˜ì˜ ì§ˆë¬¸ ìƒ˜í”Œì„ ì •ìˆ˜ ì¸ì½”ë”© : [23, 80, 69]\n",
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ë¬¸ì¥ [23, 80, 69]\n",
      "ê¸°ì¡´ ë¬¸ì¥: ë‚˜ëŠ” 2021í•™ë²ˆì´ê³  ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤í•™ë¶€ì•¼\n",
      "\n",
      "23 ----> ë‚˜ëŠ” \n",
      "80 ----> 2021í•™ë²ˆì´ê³  \n",
      "69 ----> ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤í•™ë¶€ì•¼\n"
     ]
    }
   ],
   "source": [
    "#ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”© ì‹œí—˜ìš©\n",
    "\n",
    "# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë” í† í¬ë‚˜ì´ì €ì˜ .encode()ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜.\n",
    "print('ì„ì˜ì˜ ì§ˆë¬¸ ìƒ˜í”Œì„ ì •ìˆ˜ ì¸ì½”ë”© : {}'.format(tokenizer.encode(questions[20])))\n",
    "\n",
    "\n",
    "# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë” í† í¬ë‚˜ì´ì €ì˜ .encode()ì™€ .decode() í…ŒìŠ¤íŠ¸í•´ë³´ê¸°\n",
    "# ì„ì˜ì˜ ì…ë ¥ ë¬¸ì¥ì„ sample_stringì— ì €ì¥\n",
    "sample_string = questions[20]\n",
    "\n",
    "# encode() : í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ --> ì •ìˆ˜ ì‹œí€€ìŠ¤\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ë¬¸ì¥ {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : ì •ìˆ˜ ì‹œí€€ìŠ¤ --> í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('ê¸°ì¡´ ë¬¸ì¥: {}\\n'.format(original_string))\n",
    "# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë” í† í¬ë‚˜ì´ì €ì˜ .encode()ì™€ .decode() í…ŒìŠ¤íŠ¸í•´ë³´ê¸°\n",
    "# ì„ì˜ì˜ ì…ë ¥ ë¬¸ì¥ì„ sample_stringì— ì €ì¥\n",
    "\n",
    "\n",
    "# ê° ì •ìˆ˜ëŠ” ê° ë‹¨ì–´ì™€ ì–´ë–»ê²Œ mappingë˜ëŠ”ì§€ ë³‘ë ¬ë¡œ ì¶œë ¥\n",
    "# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë”ëŠ” ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ì˜ ì„œë¸Œì›Œë“œë¡œ í† í¬ë‚˜ì´ì§•í•œë‹¤. ë„ì–´ì“°ê¸° ë‹¨ìœ„ X í˜•íƒœì†Œ ë¶„ì„ ë‹¨ìœ„ X\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de979678-1d96-49cc-80eb-d63f5277e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸ ë°ì´í„°ì˜ í¬ê¸°(shape) : (95, 40)\n",
      "ë‹µë³€ ë°ì´í„°ì˜ í¬ê¸°(shape) : (95, 40)\n"
     ]
    }
   ],
   "source": [
    "# ìµœëŒ€ ê¸¸ì´ë¥¼ 40ìœ¼ë¡œ ì •ì˜\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# í† í°í™” / ì •ìˆ˜ ì¸ì½”ë”© / ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€ / íŒ¨ë”©\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(í† í°í™” + ì •ìˆ˜ ì¸ì½”ë”©), ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # íŒ¨ë”©\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "\n",
    "\n",
    "print('ì§ˆë¬¸ ë°ì´í„°ì˜ í¬ê¸°(shape) :', questions.shape)\n",
    "print('ë‹µë³€ ë°ì´í„°ì˜ í¬ê¸°(shape) :', answers.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432d7d94-1cc6-440d-ac16-31354fbf4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ì½”ë”ì˜ ì…ë ¥ì€ ë‹µë³€ ì‹œí€€ìŠ¤ì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì œì™¸í•œ ê²ƒ\n",
    "decoder_inputs = answers[:, :-1]\n",
    "\n",
    "# ë””ì½”ë”ì˜ ì •ë‹µì€ ì²« ë²ˆì§¸ í† í°ì„ ì œì™¸í•œ ê²ƒ (í•œ ì¹¸ ì˜¤ë¥¸ìª½ìœ¼ë¡œ shift)\n",
    "decoder_targets = answers[:, 1:]\n",
    "\n",
    "#encoder_input, decoder_input, decoder_target ì´ íŠ¸ë¦¬í”Œ ì„¸íŠ¸ë¡œ ë¬¶ì—¬ì„œ, ê° ë¬¸ì¥(ìƒ˜í”Œ)ë§ˆë‹¤ í•˜ë‚˜ì”© í•™ìŠµì´ ì§„í–‰ë˜ëŠ” êµ¬ì¡°ì•¼.\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    (questions, decoder_inputs),  # input = (encoder_input, decoder_input)\n",
    "    decoder_targets\n",
    "))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000  #ì¼ë°˜ì ìœ¼ë¡œëŠ” ë°ì´í„° ì „ì²´ ìˆ˜ ì´ìƒì„ ì“°ëŠ” ê²Œ ì¢‹ì•„!\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)  #ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì–´ì„œ ê³¼ì í•©ì„ ì¤„ì´ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì„.\n",
    "dataset = dataset.batch(BATCH_SIZE)     #BATCH_SIZE ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¬¶ì–´ ëª¨ë¸ì— ì „ë‹¬\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)   #í•™ìŠµ ì¤‘ì— ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì¤€ë¹„í•´ì„œ GPUê°€ ì‰¬ì§€ ì•Šê²Œ í•¨\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. ì¸ì½”ë”ê°€ ì§ˆë¬¸ì„ ë³´ê³  ë¬¸ë§¥ íŒŒì•…\n",
    "# encoder_output = encoder(encoder_input)\n",
    "\n",
    "# 2. ë””ì½”ë”ê°€ decoder_inputì„ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡\n",
    "# decoder_output = decoder(decoder_input, encoder_output)\n",
    "\n",
    "# 3. decoder_outputì„ final dense layer í†µê³¼ â†’ ì˜ˆì¸¡ê°’ë“¤ (í™•ë¥  ë¶„í¬)\n",
    "#    ì˜ˆì¸¡ê°’ vs decoder_target ë¹„êµ â†’ loss ê³„ì‚° â†’ backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f838fa-7c02-4562-95cd-d2f068d544f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=2,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dff=512,\n",
    "    input_vocab_size=VOCAB_SIZE,\n",
    "    target_vocab_size=VOCAB_SIZE,\n",
    "    pe_input=MAX_LENGTH,\n",
    "    pe_target=MAX_LENGTH,\n",
    "    rate=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf6a8449-6213-4465-abd6-72d091fe5be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 194ms/step - accuracy_function: 0.9684 - loss: 0.0989\n",
      "Epoch 2/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy_function: 0.9673 - loss: 0.0900\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy_function: 0.9727 - loss: 0.0995\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy_function: 0.9718 - loss: 0.0997\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy_function: 0.9735 - loss: 0.0964\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - accuracy_function: 0.9712 - loss: 0.1026\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9700 - loss: 0.0954\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy_function: 0.9674 - loss: 0.0957\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy_function: 0.9734 - loss: 0.0937\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - accuracy_function: 0.9726 - loss: 0.0972\n",
      "Epoch 11/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - accuracy_function: 0.9717 - loss: 0.0946\n",
      "Epoch 12/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step - accuracy_function: 0.9718 - loss: 0.0972\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy_function: 0.9718 - loss: 0.0935\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - accuracy_function: 0.9702 - loss: 0.0974\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step - accuracy_function: 0.9751 - loss: 0.0939\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 147ms/step - accuracy_function: 0.9728 - loss: 0.0968\n",
      "Epoch 17/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9701 - loss: 0.0994\n",
      "Epoch 18/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step - accuracy_function: 0.9709 - loss: 0.0941\n",
      "Epoch 19/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy_function: 0.9692 - loss: 0.0945\n",
      "Epoch 20/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy_function: 0.9726 - loss: 0.0964\n",
      "Epoch 21/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy_function: 0.9700 - loss: 0.0961\n",
      "Epoch 22/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy_function: 0.9659 - loss: 0.0952\n",
      "Epoch 23/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy_function: 0.9728 - loss: 0.0967\n",
      "Epoch 24/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy_function: 0.9743 - loss: 0.0939\n",
      "Epoch 25/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy_function: 0.9743 - loss: 0.0944\n",
      "Epoch 26/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy_function: 0.9735 - loss: 0.0943\n",
      "Epoch 27/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - accuracy_function: 0.9692 - loss: 0.0961\n",
      "Epoch 28/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy_function: 0.9752 - loss: 0.0943\n",
      "Epoch 29/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319ms/step - accuracy_function: 0.9735 - loss: 0.0960\n",
      "Epoch 30/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy_function: 0.9683 - loss: 0.0929\n",
      "Epoch 31/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - accuracy_function: 0.9735 - loss: 0.0929\n",
      "Epoch 32/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294ms/step - accuracy_function: 0.9734 - loss: 0.0932\n",
      "Epoch 33/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy_function: 0.9797 - loss: 0.0898\n",
      "Epoch 34/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy_function: 0.9664 - loss: 0.0913\n",
      "Epoch 35/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step - accuracy_function: 0.9692 - loss: 0.0906\n",
      "Epoch 36/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy_function: 0.9735 - loss: 0.0923\n",
      "Epoch 37/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9709 - loss: 0.0928\n",
      "Epoch 38/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - accuracy_function: 0.9744 - loss: 0.0864\n",
      "Epoch 39/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - accuracy_function: 0.9751 - loss: 0.0905\n",
      "Epoch 40/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9716 - loss: 0.0861\n",
      "Epoch 41/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy_function: 0.9770 - loss: 0.0882\n",
      "Epoch 42/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - accuracy_function: 0.9726 - loss: 0.0860\n",
      "Epoch 43/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9777 - loss: 0.0871\n",
      "Epoch 44/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9771 - loss: 0.0883\n",
      "Epoch 45/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9777 - loss: 0.0881\n",
      "Epoch 46/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy_function: 0.9794 - loss: 0.0857\n",
      "Epoch 47/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9776 - loss: 0.0846\n",
      "Epoch 48/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy_function: 0.9802 - loss: 0.0848\n",
      "Epoch 49/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy_function: 0.9734 - loss: 0.0819\n",
      "Epoch 50/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9779 - loss: 0.0885\n",
      "Epoch 51/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - accuracy_function: 0.9769 - loss: 0.0875\n",
      "Epoch 52/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy_function: 0.9787 - loss: 0.0852\n",
      "Epoch 53/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy_function: 0.9747 - loss: 0.0804\n",
      "Epoch 54/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9802 - loss: 0.0864\n",
      "Epoch 55/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy_function: 0.9769 - loss: 0.0796\n",
      "Epoch 56/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy_function: 0.9726 - loss: 0.0827\n",
      "Epoch 57/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9777 - loss: 0.0828\n",
      "Epoch 58/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy_function: 0.9797 - loss: 0.0842\n",
      "Epoch 59/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy_function: 0.9786 - loss: 0.0800\n",
      "Epoch 60/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9783 - loss: 0.0831\n",
      "Epoch 61/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy_function: 0.9829 - loss: 0.0785\n",
      "Epoch 62/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy_function: 0.9785 - loss: 0.0779\n",
      "Epoch 63/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy_function: 0.9795 - loss: 0.0779\n",
      "Epoch 64/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy_function: 0.9812 - loss: 0.0773\n",
      "Epoch 65/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy_function: 0.9786 - loss: 0.0764\n",
      "Epoch 66/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy_function: 0.9744 - loss: 0.0743\n",
      "Epoch 67/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy_function: 0.9771 - loss: 0.0785\n",
      "Epoch 68/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy_function: 0.9786 - loss: 0.0727\n",
      "Epoch 69/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9828 - loss: 0.0741\n",
      "Epoch 70/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy_function: 0.9820 - loss: 0.0737\n",
      "Epoch 71/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 181ms/step - accuracy_function: 0.9820 - loss: 0.0743\n",
      "Epoch 72/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy_function: 0.9802 - loss: 0.0700\n",
      "Epoch 73/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - accuracy_function: 0.9802 - loss: 0.0741\n",
      "Epoch 74/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9804 - loss: 0.0734\n",
      "Epoch 75/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9795 - loss: 0.0743\n",
      "Epoch 76/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy_function: 0.9838 - loss: 0.0752\n",
      "Epoch 77/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9838 - loss: 0.0714\n",
      "Epoch 78/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9834 - loss: 0.0669\n",
      "Epoch 79/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 0.9811 - loss: 0.0669\n",
      "Epoch 80/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy_function: 0.9828 - loss: 0.0712\n",
      "Epoch 81/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy_function: 0.9854 - loss: 0.0693\n",
      "Epoch 82/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy_function: 0.9864 - loss: 0.0705\n",
      "Epoch 83/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9838 - loss: 0.0710\n",
      "Epoch 84/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy_function: 0.9854 - loss: 0.0703\n",
      "Epoch 85/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219ms/step - accuracy_function: 0.9889 - loss: 0.0649\n",
      "Epoch 86/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy_function: 0.9863 - loss: 0.0661\n",
      "Epoch 87/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9846 - loss: 0.0621\n",
      "Epoch 88/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy_function: 0.9829 - loss: 0.0644\n",
      "Epoch 89/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy_function: 0.9871 - loss: 0.0641\n",
      "Epoch 90/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9872 - loss: 0.0596\n",
      "Epoch 91/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9829 - loss: 0.0654\n",
      "Epoch 92/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - accuracy_function: 0.9830 - loss: 0.0640\n",
      "Epoch 93/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9855 - loss: 0.0592\n",
      "Epoch 94/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9846 - loss: 0.0611\n",
      "Epoch 95/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9905 - loss: 0.0552\n",
      "Epoch 96/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy_function: 0.9889 - loss: 0.0587\n",
      "Epoch 97/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy_function: 0.9906 - loss: 0.0589\n",
      "Epoch 98/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy_function: 0.9890 - loss: 0.0596\n",
      "Epoch 99/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9897 - loss: 0.0599\n",
      "Epoch 100/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy_function: 0.9863 - loss: 0.0605\n",
      "Epoch 101/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy_function: 0.9871 - loss: 0.0567\n",
      "Epoch 102/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy_function: 0.9872 - loss: 0.0565\n",
      "Epoch 103/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy_function: 0.9906 - loss: 0.0561\n",
      "Epoch 104/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy_function: 0.9914 - loss: 0.0559\n",
      "Epoch 105/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy_function: 0.9897 - loss: 0.0542\n",
      "Epoch 106/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 221ms/step - accuracy_function: 0.9880 - loss: 0.0528\n",
      "Epoch 107/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy_function: 0.9888 - loss: 0.0546\n",
      "Epoch 108/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy_function: 0.9898 - loss: 0.0552\n",
      "Epoch 109/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy_function: 0.9915 - loss: 0.0522\n",
      "Epoch 110/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328ms/step - accuracy_function: 0.9914 - loss: 0.0478\n",
      "Epoch 111/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy_function: 0.9889 - loss: 0.0520\n",
      "Epoch 112/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9924 - loss: 0.0512\n",
      "Epoch 113/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy_function: 0.9897 - loss: 0.0532\n",
      "Epoch 114/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - accuracy_function: 0.9941 - loss: 0.0503\n",
      "Epoch 115/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy_function: 0.9940 - loss: 0.0504\n",
      "Epoch 116/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy_function: 0.9906 - loss: 0.0505\n",
      "Epoch 117/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9922 - loss: 0.0469\n",
      "Epoch 118/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 0.9906 - loss: 0.0497\n",
      "Epoch 119/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9916 - loss: 0.0458\n",
      "Epoch 120/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy_function: 0.9941 - loss: 0.0475\n",
      "Epoch 121/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9949 - loss: 0.0460\n",
      "Epoch 122/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy_function: 0.9923 - loss: 0.0478\n",
      "Epoch 123/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - accuracy_function: 0.9940 - loss: 0.0447\n",
      "Epoch 124/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9923 - loss: 0.0474\n",
      "Epoch 125/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy_function: 0.9915 - loss: 0.0445\n",
      "Epoch 126/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy_function: 0.9915 - loss: 0.0432\n",
      "Epoch 127/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy_function: 0.9941 - loss: 0.0425\n",
      "Epoch 128/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy_function: 0.9932 - loss: 0.0396\n",
      "Epoch 129/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9966 - loss: 0.0413\n",
      "Epoch 130/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9966 - loss: 0.0401\n",
      "Epoch 131/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9931 - loss: 0.0426\n",
      "Epoch 132/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy_function: 0.9923 - loss: 0.0404\n",
      "Epoch 133/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 0.9940 - loss: 0.0383\n",
      "Epoch 134/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy_function: 0.9957 - loss: 0.0373\n",
      "Epoch 135/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9966 - loss: 0.0384\n",
      "Epoch 136/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy_function: 0.9957 - loss: 0.0387\n",
      "Epoch 137/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy_function: 0.9949 - loss: 0.0361\n",
      "Epoch 138/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy_function: 0.9974 - loss: 0.0370\n",
      "Epoch 139/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy_function: 0.9974 - loss: 0.0364\n",
      "Epoch 140/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy_function: 0.9957 - loss: 0.0362\n",
      "Epoch 141/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388ms/step - accuracy_function: 0.9966 - loss: 0.0384\n",
      "Epoch 142/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy_function: 0.9982 - loss: 0.0332\n",
      "Epoch 143/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9966 - loss: 0.0347\n",
      "Epoch 144/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy_function: 0.9992 - loss: 0.0334\n",
      "Epoch 145/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy_function: 0.9974 - loss: 0.0331\n",
      "Epoch 146/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy_function: 0.9966 - loss: 0.0330\n",
      "Epoch 147/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy_function: 0.9974 - loss: 0.0332\n",
      "Epoch 148/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9992 - loss: 0.0301\n",
      "Epoch 149/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy_function: 0.9957 - loss: 0.0332\n",
      "Epoch 150/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy_function: 0.9974 - loss: 0.0312\n",
      "Epoch 151/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343ms/step - accuracy_function: 0.9983 - loss: 0.0311\n",
      "Epoch 152/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy_function: 0.9966 - loss: 0.0287\n",
      "Epoch 153/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 0.9991 - loss: 0.0276\n",
      "Epoch 154/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy_function: 0.9974 - loss: 0.0268\n",
      "Epoch 155/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9966 - loss: 0.0296\n",
      "Epoch 156/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9957 - loss: 0.0279\n",
      "Epoch 157/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy_function: 0.9983 - loss: 0.0275\n",
      "Epoch 158/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - accuracy_function: 0.9967 - loss: 0.0261\n",
      "Epoch 159/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9992 - loss: 0.0268\n",
      "Epoch 160/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9992 - loss: 0.0275\n",
      "Epoch 161/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy_function: 0.9992 - loss: 0.0252\n",
      "Epoch 162/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - accuracy_function: 0.9974 - loss: 0.0254\n",
      "Epoch 163/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy_function: 0.9992 - loss: 0.0253\n",
      "Epoch 164/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy_function: 0.9991 - loss: 0.0253\n",
      "Epoch 165/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy_function: 0.9983 - loss: 0.0234\n",
      "Epoch 166/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy_function: 0.9982 - loss: 0.0235\n",
      "Epoch 167/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9992 - loss: 0.0252\n",
      "Epoch 168/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9991 - loss: 0.0229\n",
      "Epoch 169/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy_function: 0.9992 - loss: 0.0236\n",
      "Epoch 170/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy_function: 0.9983 - loss: 0.0222\n",
      "Epoch 171/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9991 - loss: 0.0223\n",
      "Epoch 172/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy_function: 0.9992 - loss: 0.0221\n",
      "Epoch 173/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy_function: 0.9983 - loss: 0.0216\n",
      "Epoch 174/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 1.0000 - loss: 0.0204\n",
      "Epoch 175/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9983 - loss: 0.0222\n",
      "Epoch 176/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy_function: 0.9983 - loss: 0.0190\n",
      "Epoch 177/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 1.0000 - loss: 0.0210\n",
      "Epoch 178/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy_function: 1.0000 - loss: 0.0214\n",
      "Epoch 179/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy_function: 1.0000 - loss: 0.0195\n",
      "Epoch 180/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy_function: 0.9991 - loss: 0.0199\n",
      "Epoch 181/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9983 - loss: 0.0182\n",
      "Epoch 182/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - accuracy_function: 1.0000 - loss: 0.0183\n",
      "Epoch 183/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy_function: 0.9992 - loss: 0.0178\n",
      "Epoch 184/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy_function: 0.9983 - loss: 0.0186\n",
      "Epoch 185/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - accuracy_function: 0.9992 - loss: 0.0173\n",
      "Epoch 186/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy_function: 1.0000 - loss: 0.0170\n",
      "Epoch 187/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 1.0000 - loss: 0.0179\n",
      "Epoch 188/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - accuracy_function: 1.0000 - loss: 0.0172\n",
      "Epoch 189/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy_function: 1.0000 - loss: 0.0164\n",
      "Epoch 190/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy_function: 1.0000 - loss: 0.0154\n",
      "Epoch 191/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy_function: 0.9992 - loss: 0.0168\n",
      "Epoch 192/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy_function: 1.0000 - loss: 0.0154\n",
      "Epoch 193/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 1.0000 - loss: 0.0152\n",
      "Epoch 194/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy_function: 0.9991 - loss: 0.0150\n",
      "Epoch 195/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 1.0000 - loss: 0.0143\n",
      "Epoch 196/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy_function: 1.0000 - loss: 0.0144\n",
      "Epoch 197/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy_function: 1.0000 - loss: 0.0136\n",
      "Epoch 198/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - accuracy_function: 1.0000 - loss: 0.0129\n",
      "Epoch 199/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy_function: 1.0000 - loss: 0.0134\n",
      "Epoch 200/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy_function: 0.9991 - loss: 0.0136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f330730c80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200  # ì›í•˜ëŠ” ë§Œí¼ ë°˜ë³µ\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # í•™ìŠµë¥  ê³„ì‚° ê³µì‹\n",
    "        \n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # íŒ¨ë”© ì œì™¸í•œ í† í°ë§Œ ê³„ì‚°\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "learning_rate = CustomSchedule(128)  # ë„¤ ëª¨ë¸ì—ì„œ d_model ì‚¬ìš©í–ˆì„ ê±°ì•¼\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    real = tf.cast(real, tf.int64)\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
    "\n",
    "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])\n",
    "\n",
    "transformer.fit(dataset, epochs=EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001946fe-c187-430b-8541-4c97a9bc7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  # ë‹¨ì–´ì™€ êµ¬ë‘ì  ì‚¬ì´ì— ê³µë°± ì¶”ê°€.\n",
    "  # ex) 12ì‹œ ë•¡! -> 12ì‹œ ë•¡ !\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "\n",
    "def evaluate(sentence):\n",
    "  # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ ì „ì²˜ë¦¬\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # ì…ë ¥ ë¬¸ì¥ì— ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
    "  sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ ì˜ˆì¸¡ ì‹œì‘\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = transformer(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë°›ì•„ì˜¨ë‹¤.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # ë§Œì•½ í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ ì˜ˆì¸¡ì„ ì¤‘ë‹¨\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ output(ì¶œë ¥)ì— ì—°ê²°í•œë‹¤.\n",
    "    # outputì€ forë¬¸ì˜ ë‹¤ìŒ ë£¨í”„ì—ì„œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ëœë‹¤.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  # ë‹¨ì–´ ì˜ˆì¸¡ì´ ëª¨ë‘ ëë‚¬ë‹¤ë©´ outputì„ ë¦¬í„´.\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  # prediction == ë””ì½”ë”ê°€ ë¦¬í„´í•œ ì±—ë´‡ì˜ ëŒ€ë‹µì— í•´ë‹¹í•˜ëŠ” ì •ìˆ˜ ì‹œí€€ìŠ¤\n",
    "  # tokenizer.decode()ë¥¼ í†µí•´ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë””ì½”ë”©.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778fc1cc-adee-44ea-b1d9-b1063e16be8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m predict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì•ˆë…•\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(sentence):\n\u001b[1;32m---> 38\u001b[0m   prediction \u001b[38;5;241m=\u001b[39m evaluate(sentence)\n\u001b[0;32m     40\u001b[0m   \u001b[38;5;66;03m# prediction == ë””ì½”ë”ê°€ ë¦¬í„´í•œ ì±—ë´‡ì˜ ëŒ€ë‹µì— í•´ë‹¹í•˜ëŠ” ì •ìˆ˜ ì‹œí€€ìŠ¤\u001b[39;00m\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;66;03m# tokenizer.decode()ë¥¼ í†µí•´ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë””ì½”ë”©.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m   predicted_sentence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m     43\u001b[0m       [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m prediction \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mvocab_size])\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(sentence):\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;66;03m# ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ ì „ì²˜ë¦¬\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m preprocess_sentence(sentence)\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;66;03m# ì…ë ¥ ë¬¸ì¥ì— ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\u001b[39;00m\n\u001b[0;32m     13\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(\n\u001b[0;32m     14\u001b[0m       START_TOKEN \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(sentence) \u001b[38;5;241m+\u001b[39m END_TOKEN, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mpreprocess_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_sentence\u001b[39m(sentence):\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;66;03m# ë‹¨ì–´ì™€ êµ¬ë‘ì  ì‚¬ì´ì— ê³µë°± ì¶”ê°€.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;66;03m# ex) 12ì‹œ ë•¡! -> 12ì‹œ ë•¡ !\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m([?.!,])\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\"\u001b[39m, sentence)\n\u001b[0;32m      5\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sentence\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "output = predict(\"ì•ˆë…•\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1aa07-d92b-4707-8e2f-254c3da817d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
