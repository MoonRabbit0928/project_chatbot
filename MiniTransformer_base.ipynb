{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11f3463b-6204-4dc7-97e1-26c993c44c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = pos / tf.pow(10000, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        return angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            pos=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        return pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(q, k, v, mask):              #(Q, K, V는 다 4D 텐서야: (batch_size, num_heads, seq_len, depth))\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(layers.Layer):        #num_head = 헤드개수 <예)8개>\n",
    "    def __init__(self, d_model, num_heads):   #d_model: 전체 출력 차원 (예: 512)\n",
    "        super().__init__()                     #전체 차원이 head 수로 정확히 나눠떨어져야 해.\n",
    "        assert d_model % num_heads == 0         #예를 들면 d_model=512, num_heads=8이면 head당 depth는 512/8=64야.\n",
    "\n",
    "        self.num_heads = num_heads           # 헤드 개수 저장\n",
    "        self.depth = d_model // num_heads     # 깊이 저장\n",
    "\n",
    "        self.wq = layers.Dense(d_model)      # Query, Key, Value를 각각 Dense 레이어로 변환할 준비\n",
    "        self.wk = layers.Dense(d_model)      #각각 입력을 d_model 차원으로 변환시켜.\n",
    "        self.wv = layers.Dense(d_model)      #여러 head를 만들려면 Q, K, V를 한번 linear transform 해줘야 함. (독립적으로 head들이 일할 수 있도록)\n",
    "\n",
    "        self.dense = layers.Dense(d_model)   #마지막에 여러 head를 이어붙인 걸 다시 d_model 크기로 맞춰주는 Dense 레이어.\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        ## 1. (batch_size, seq_len, d_model) → (batch_size, seq_len, num_heads, depth)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  #x를 (batch_size, seq_len, num_heads, depth)로 reshape.\n",
    "        # -1은 seq_len 같은 애들이 고정이 아닐 때 자동으로 맞춰주려고 쓰는 거야!\n",
    "        \n",
    "        # 2. (batch_size, seq_len, num_heads, depth) → (batch_size, num_heads, seq_len, depth)\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])                        \n",
    "                                                                         #그리고 num_heads 차원이 앞으로 오게 transpose.\n",
    "                                                                         #최종 shape: (batch_size, num_heads, seq_len, depth)\n",
    "                                                                         #멀티헤드니까, head별로 독립적으로 attention을 계산하려고 이렇게 나눔.\n",
    "                                                                                 \n",
    "    \n",
    "    def call(self, v, k, q, attention_mask=None, training=False):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)   #입력 q, k, v에 대해 Dense 통과시켜줌.\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  #그리고 각각을 멀티헤드 형태로 쪼갬.\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, attention_mask)            #각각의 head에 대해 Scaled Dot-Product Attention 적용.\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])         #q, k, v가 멀티헤드 모양이라 병렬로 적용됨.\n",
    "    \n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
    "        #head들을 하나로 붙임 (concat)\n",
    "        #shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  #마지막 Dense를 거쳐서 최종 output 만듦.\n",
    "        return output\n",
    "       #[입력 Q, K, V] \n",
    "       #  -> (각각 Dense에 통과) \n",
    "       #  -> (split_heads로 여러 head로 나눔)\n",
    "       #  -> (각 head마다 scaled dot-product attention 계산)\n",
    "       #  -> (head들 concat)\n",
    "       #  -> (마지막 Dense로 출력)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------- 인코더와 디코더 구조 ------------------------------------#\n",
    "\n",
    "#📘 Encoder Layer\n",
    "#[Input] \n",
    "# └▶ MultiHeadAttention(Self)\n",
    "#└▶ Add & Norm\n",
    "# └▶ Feed Forward\n",
    "# └▶ Add & Norm\n",
    "# └▶ Output to next encoder block\n",
    "\n",
    "#📗 Decoder Layer\n",
    "#[Previous Output + Encoder Output]\n",
    "# └▶ MultiHeadAttention(Self + Mask)\n",
    "# └▶ Add & Norm\n",
    "# └▶ MultiHeadAttention(from Encoder Output)\n",
    "# └▶ Add & Norm\n",
    "# └▶ Feed Forward\n",
    "# └▶ Add & Norm\n",
    "# └▶ Output to next decoder block\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(layers.Layer):                             ## <dff와 rate는 하이퍼파라미터!!>\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):    ## d_model: 임베딩 차원. 보통 128, 256, 512, 768 같은 2^n 값.\n",
    "        super().__init__()                                    ## dff: FFN 내부 은닉층의 차원. 보통 d_model * 2 혹은 d_model * 4 정도.\n",
    "                                                              ##  rate: 드롭아웃 확률. 0.1이면 10%를 랜덤으로 끊어.\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  \n",
    "\n",
    "        self.ffn = tf.keras.Sequential([            ### Feed Forward MLP 은닉층 한개, 출력층 한개. ==> 가중치 줘서 차원 높였다가 줄임. \n",
    "            layers.Dense(dff, activation='relu'),   ### 차원을 높였다가 줄이는 이유는 정보 표현 능력을 높이기 위함이다.\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        ## 층 정규화                                                  #epsilon=1e-6: 분모가 0 되는 걸 방지하는 아주 작은 값.\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)    #미니배치 단위가 아니라 피처 단위로 정규화한다.\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)    #각 위치의 임베딩 벡터 (예: [0.2, -0.4, 1.3, ...]) 를 평균 0, 분산 1로 정규화.\n",
    "\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)                         #드롭아웃 층 두개 \n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None, training=False):                                    \n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)                               #1.Self-Attention 수행\n",
    "        attn_output = self.dropout1(attn_output, training=training)         #2.드롭아웃 ==> 활성화 함수가 없는 은닉층처럼 활용\n",
    "        out1 = self.layernorm1(x + attn_output)                             #3.잔차연결 및 정규화 <여기서 잔차연결이란 input + attn_output인데 >\n",
    " #<여기서 잔차연결이란 input + attn_output인데 정보손실 방지 역할을 한다 그냥 외워>\n",
    "        \n",
    "        ffn_output = self.ffn(out1)                                         #4.FFN 통과 ==> 드롭아웃을 거친 뉴런들이 FFN을 통과하면서 다시 비중이 생김\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)           #5.다시 드롭아웃 => 다시 활성화 함수가 없는 은닉층 처럼 활용\n",
    "        out2 = self.layernorm2(out1 + ffn_output)                           #6.잔차연결 및 정규화 ==>끝\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n",
    "        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)                      #1단계: Masked Self-Attention\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask)    #2단계: Encoder-Decoder Attention\n",
    "        attn2 = self.dropout2(attn2, training=training)                  #Query: 디코더가 현재까지 만든 출력 (디코더의 상태)\n",
    "        out2 = self.layernorm2(attn2 + out1)                             #Key, Value: 인코더의 출력 (전체 문장의 의미 벡터들)\n",
    "                                                                         #이걸로 어텐션 → context vector를 구함\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3\n",
    "\n",
    "\n",
    "#<디코더 레이어 전체 그림>\n",
    "\n",
    "#Input x\n",
    "#  ↓\n",
    "#Masked MultiHead Attention (Q=K=V=x, look-ahead mask)\n",
    "#  ↓\n",
    "#Residual + LayerNorm\n",
    "#  ↓\n",
    "#MultiHead Attention with Encoder Output (Q=Decoder, K=V=Encoder)\n",
    "#  ↓\n",
    "#Residual + LayerNorm\n",
    "#  ↓\n",
    "#Feed Forward Network (Dense → ReLU → Dense)\n",
    "#  ↓\n",
    "#Residual + LayerNorm\n",
    "#  ↓\n",
    "#Output (x의 디코더 처리 결과)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#항목\t     Encoder의 Multi-Head Attention\t          vs               Decoder의 Masked Multi-Head Self-Attention\n",
    "\n",
    "#목적\t     입력 전체를 스스로 이해 (글의 의미 파악)\t                   이전 단어만 보고 다음 단어 예측\n",
    "#마스크\t     Padding mask만 사용\t                                       Padding mask + Look-ahead mask 사용\n",
    "#정보 접근\t 모든 토큰에 자유롭게 접근\t                               자기 앞의 단어까지만 접근 가능 (미래 차단)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Encoder\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                                                ##maximum_position_encoding : 입력 문장의 최대 길이를 의미해!\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers     ## 인코딩 층을 얼마나 통과시킬 건지 => 보통 6개를 많이 쓴다고 한다.\n",
    "\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))   \n",
    "        #이건 임베딩된 벡터의 스케일을 조정해주는 코드야. 멀티헤드어텐션과 다른 점은 스케일을 키워준다!!\n",
    "        \n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for enc_layer in self.enc_layers:           ##\n",
    "            x = enc_layer(x, mask=mask, training=training)\n",
    "\n",
    "        return x        #최종 출력 x는 크기가 (batch_size, input_seq_len, d_model)인 텐서\n",
    "                        #이걸 디코더에서 키-값 벡터로 참조해서 예측에 사용해 ==>> 인코더-디코더 어텐션에서의 KEY, VALUE로 input되는 값이 된다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 100자 이내의 짧은 문장만 다룬다면\n",
    "#maximum_position_encoding = 100\n",
    "\n",
    "# 문장 번역용 일반 Transformer라면\n",
    "#maximum_position_encoding = 512\n",
    "\n",
    "# 논문 전체처럼 긴 문서를 처리할 거라면\n",
    "#maximum_position_encoding = 1024 ~ 4096\n",
    "\n",
    "# Decoder\n",
    "class Decoder(layers.Layer):            #target_vocab_size : 예측할 대상 단어 집합의 크기  ///  maximum_position_encoding\t: 위치 인코딩에서 최대 문장길이\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                 #dff => FFN 은닉층 차원 \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)   \n",
    "        #디코더의 embedding()은 **\"정답 문장\"의 앞부분(예측해야 할 문장의 이전 단어들)**을 임베딩하는 거야.\n",
    "\n",
    "#단계\t                     입력값\t                        설명\n",
    "#Encoder input\t             \"나는 밥을 먹었다\"\t            인코더에 들어갈 입력 문장 (source)\n",
    "#Decoder input\t             [\"<s>\", \"I\", \"ate\"]\t        디코더에 들어갈 입력 (target의 앞부분)\n",
    "#Decoder output (target)\t [\"I\", \"ate\", \"rice\"]\t        디코더가 예측해야 하는 실제 정답\n",
    "       \n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)   \n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for dec_layer in self.dec_layers:\n",
    "            x = dec_layer(x, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask, training=training)\n",
    "\n",
    "        return x   #(batch_size, target_seq_len, target_vocab_size)   #디코더 각 위치에서 최종적으로 만들어낸 벡터들\n",
    "\n",
    "\n",
    "\n",
    "# Transformer\n",
    "\n",
    "class Transformer(tf.keras.Model):   \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()   \n",
    "        #input_vocab_size:인코더에 들어갈 문장의 단어 집합 크기 (vocabulary size)\n",
    "        #target_vocab_size : 디코더가 생성할 문장의 단어 집합 크기\n",
    "        #pe_input : Positional Encoding을 위한 입력 시퀀스 최대 길이, 예: 입력 문장의 최대 길이가 100이라면 → pe_input = 100\n",
    "        #pe_target : 디코더 입력 (정답 문장)의 최대 길이, 예: : 정답 문장의 최대 길이가 80이라면 → pe_target = 80\n",
    "        #주의: pe_input, pe_target은 학습 데이터보다 충분히 크게 잡자! 최대 문장 길이보다 살짝 크게 잡아두는 게 안전함 (ex. 128, 256 등)\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # unpacking\n",
    "        inp, tar = inputs\n",
    "\n",
    "        # 마스크 생성\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "\n",
    "        # 인코더-디코더 흐름\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask, training=training)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output\n",
    "        #로짓 (logits): 아직 softmax는 안 씌운 생 확률 점수들. \n",
    "        #보통 loss 계산할 때 softmax 없이 logits을 쓰는 게 더 안정적이기 때문에 이렇게 반환해.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#Input Sentence  --->  Encoder ----------------------------┐\n",
    "#                    ↓                                     ↓\n",
    "#               Encoded Context  -->  Decoder (with target input)\n",
    "#                                          ↓\n",
    "#                                 Final Dense (Softmax logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\"입력 토큰 ID\" = 정수 인코딩된 값\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 요약 흐름\n",
    "#입력문장 inp → 인코더에 들어감 → enc_output 생성\n",
    "\n",
    "#정답 문장 tar → 디코더에 들어감 (enc_output 참고함)\n",
    "\n",
    "#디코더 출력 dec_output → Dense layer → 단어 확률 분포 final_output\n",
    "\n",
    "#리턴: (batch, target_seq_len, vocab_size) 형태의 로짓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cd1620-748f-48b0-9edc-62586223ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = pos / tf.pow(10000, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        return angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            pos=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        return pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):              \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):       \n",
    "    def __init__(self, d_model, num_heads):   \n",
    "        super().__init__()                     \n",
    "        assert d_model % num_heads == 0        \n",
    "\n",
    "        self.num_heads = num_heads           \n",
    "        self.depth = d_model // num_heads   \n",
    "\n",
    "        self.wq = layers.Dense(d_model)     \n",
    "        self.wk = layers.Dense(d_model)      \n",
    "        self.wv = layers.Dense(d_model)     \n",
    "\n",
    "        self.dense = layers.Dense(d_model)   \n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, v, k, q, attention_mask=None, training=False):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)   #입력 q, k, v에 대해 Dense 통과시켜줌.\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  #그리고 각각을 멀티헤드 형태로 쪼갬.\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, attention_mask)            \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])         \n",
    "    \n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
    "        output = self.dense(concat_attention)  \n",
    "        return output\n",
    "\n",
    "class EncoderLayer(layers.Layer):                           \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):    \n",
    "        super().__init__()                                    \n",
    "                                                              \n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  \n",
    "\n",
    "        self.ffn = tf.keras.Sequential([          \n",
    "            layers.Dense(dff, activation='relu'),   \n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        ## 층 정규화                                                  \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)    \n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)    \n",
    "\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)                         \n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None, training=False):                                    \n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)                               \n",
    "        attn_output = self.dropout1(attn_output, training=training)        \n",
    "        out1 = self.layernorm1(x + attn_output)                             \n",
    "        \n",
    "        ffn_output = self.ffn(out1)                                         \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)           \n",
    "        out2 = self.layernorm2(out1 + ffn_output)                           \n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n",
    "        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)                 \n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask)    \n",
    "        attn2 = self.dropout2(attn2, training=training)                  \n",
    "        out2 = self.layernorm2(attn2 + out1)                             \n",
    "                                                                         \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                                             \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers \n",
    "\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))   \n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for enc_layer in self.enc_layers:           \n",
    "            x = enc_layer(x, mask=mask, training=training)\n",
    "\n",
    "        return x  \n",
    "\n",
    "class Decoder(layers.Layer):            \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                 \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)        \n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)   \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for dec_layer in self.dec_layers:\n",
    "            x = dec_layer(x, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(tf.keras.Model):   \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()   \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        inp, tar = inputs\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask, training=training)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d92387f-465d-4131-9f6a-c902ddd0137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = pos / tf.pow(10000, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        return angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            pos=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        return pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):              \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):       \n",
    "    def __init__(self, d_model, num_heads):   \n",
    "        super().__init__()                     \n",
    "        assert d_model % num_heads == 0        \n",
    "\n",
    "        self.num_heads = num_heads           \n",
    "        self.depth = d_model // num_heads   \n",
    "\n",
    "        self.wq = layers.Dense(d_model)     \n",
    "        self.wk = layers.Dense(d_model)      \n",
    "        self.wv = layers.Dense(d_model)     \n",
    "\n",
    "        self.dense = layers.Dense(d_model)   \n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, v, k, q, attention_mask=None, training=False):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)   #입력 q, k, v에 대해 Dense 통과시켜줌.\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  #그리고 각각을 멀티헤드 형태로 쪼갬.\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, attention_mask)            \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])         \n",
    "    \n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
    "        output = self.dense(concat_attention)  \n",
    "        return output\n",
    "\n",
    "class EncoderLayer(layers.Layer):                           \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):    \n",
    "        super().__init__()                                    \n",
    "                                                              \n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)  \n",
    "\n",
    "        self.ffn = tf.keras.Sequential([          \n",
    "            layers.Dense(dff, activation='relu'),   \n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        ## 층 정규화                                                  \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)    \n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)    \n",
    "\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)                         \n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None, training=False):                                    \n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)                               \n",
    "        attn_output = self.dropout1(attn_output, training=training)        \n",
    "        out1 = self.layernorm1(x + attn_output)                             \n",
    "        \n",
    "        ffn_output = self.ffn(out1)                                         \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)           \n",
    "        out2 = self.layernorm2(out1 + ffn_output)                           \n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n",
    "        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)                 \n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask)    \n",
    "        attn2 = self.dropout2(attn2, training=training)                  \n",
    "        out2 = self.layernorm2(attn2 + out1)                             \n",
    "                                                                         \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                                             \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers \n",
    "\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))   \n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for enc_layer in self.enc_layers:           \n",
    "            x = enc_layer(x, mask=mask, training=training)\n",
    "\n",
    "        return x  \n",
    "\n",
    "class Decoder(layers.Layer):            \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super().__init__()                 \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)        \n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)   \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for dec_layer in self.dec_layers:\n",
    "            x = dec_layer(x, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(tf.keras.Model):   \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()   \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        inp, tar = inputs\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask, training=training)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debcf9c-95c3-42be-855d-15391aea2745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3ce3f-7668-487d-ba99-1777ef083a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7eb0d-1ddc-4186-8002-af827ac98ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90ea6d-f886-419a-a8b0-7bf326755219",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar 에 관한 것\n",
    "상황\t       디코더 입력\t                        예측 방식\t                Mask\n",
    "훈련\t    [\"<start>\", \"I\", \"love\", \"you\"]\t        병렬로 전체 예측\t            ✅ Look-ahead mask 사용\n",
    "추론(예측)\t[\"<start>\"] → [\"<start>\", \"I\"] → ...\t순차적으로 한 토큰씩 예측\t    ✅ Look-ahead mask 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99803248-66ae-426e-b7ed-87c707dedb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f02e8a-0ae3-441d-9064-f68556fa11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지 트랜스포머를 이해하기 위한 미니트랜스포머의 구현이 끝났다.\n",
    "# 트랜스포머 구현을 했고 이를 이해하기 위해 노력했지만, 아마 완전히 이해하지는 못했을 것이다.\n",
    "# 이는 수많은 연습과 나만의 프로젝트를 구현하면서 극복할 것으로 예상된다.\n",
    "# 토스 ceo가 열심히 노력하면 언젠간 기회는 찾아온다고 했다. 난 그 기회를 잡기 위해서 열심히 노력하고 있는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3395d-e9a0-4fbd-8542-6f9ef64e86f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8be6a9-5fbf-4480-9ddf-e6c8cd515eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#트랜스포머 기반 챗봇\n",
    "\n",
    "✅ 챗봇 구현 흐름 (Transformer 기반)\n",
    "1.데이터 준비\n",
    "\n",
    "  질문과 답변 쌍으로 구성된 대화 데이터셋 (ex. “졸업 요건이 뭐야?” → “총 130학점이 필요해요”)\n",
    "\n",
    "  정수 인코딩 (tokenizer 사용)\n",
    "\n",
    "2.입출력 데이터 전처리\n",
    "\n",
    "  패딩 / 마스킹 / 시작 토큰 (<start>) / 종료 토큰 (<end>) 붙이기\n",
    "\n",
    "  inp → encoder 입력\n",
    "\n",
    "  tar_input → decoder 입력\n",
    "\n",
    "  tar_real → decoder 출력(정답)\n",
    "\n",
    "3.트랜스포머 모델 구현\n",
    "\n",
    "  방금 만든 Transformer 클래스 사용\n",
    "\n",
    "  loss function, optimizer 정의\n",
    "\n",
    "4.모델 학습\n",
    "\n",
    "  teacher forcing 기법 적용\n",
    "\n",
    "  여러 epoch 동안 학습\n",
    "\n",
    "5.챗봇 예측 (inference)\n",
    "\n",
    "  하나의 질문 입력 → decoder가 한 글자씩 자동으로 다음 단어 예측 (auto-regressive)\n",
    "\n",
    "  <end> 토큰이 나올 때까지 반복\n",
    "\n",
    "6.CLI 기반 챗봇 만들기\n",
    "\n",
    "  사용자 입력 받기 → 모델 응답 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f2a4a-6a50-4a7d-b7d3-bd2695a7e310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea023f76-feb7-4950-9db9-3a5ed11968e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87fb6328-17be-4e95-889d-31be1bfa6325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>Q_clean</th>\n",
       "      <th>A</th>\n",
       "      <th>A_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>졸업요건 알려줘</td>\n",
       "      <td>졸업요건 알려줘</td>\n",
       "      <td>졸업요건을 알려면 학번과 학과를 알려줘.</td>\n",
       "      <td>졸업요건을 알려면 학번과 학과를 알려줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어학 기준이 궁금해</td>\n",
       "      <td>어학 기준이 궁금해</td>\n",
       "      <td>어학 기준은 학번별로 달라. 학번을 알려줄래?</td>\n",
       "      <td>어학 기준은 학번별로 달라 학번을 알려줄래</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>논문 꼭 써야 해?</td>\n",
       "      <td>논문 꼭 써야 해</td>\n",
       "      <td>논문 대신 졸업 프로젝트나 포트폴리오로 대체할 수 있어.</td>\n",
       "      <td>논문 대신 졸업 프로젝트나 포트폴리오로 대체할 수 있어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>총 몇 학점 들어야 해?</td>\n",
       "      <td>총 몇 학점 들어야 해</td>\n",
       "      <td>총 130학점 이상 이수해야 졸업할 수 있어.</td>\n",
       "      <td>총 130학점 이상 이수해야 졸업할 수 있어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>교양필수 과목 알려줘</td>\n",
       "      <td>교양필수 과목 알려줘</td>\n",
       "      <td>대학공통교양으로는 글쓰기, 영어, 컴퓨팅사고가 포함돼.</td>\n",
       "      <td>대학공통교양으로는 글쓰기 영어 컴퓨팅사고가 포함돼</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Q       Q_clean                                A  \\\n",
       "0       졸업요건 알려줘      졸업요건 알려줘           졸업요건을 알려면 학번과 학과를 알려줘.   \n",
       "1     어학 기준이 궁금해    어학 기준이 궁금해        어학 기준은 학번별로 달라. 학번을 알려줄래?   \n",
       "2     논문 꼭 써야 해?     논문 꼭 써야 해  논문 대신 졸업 프로젝트나 포트폴리오로 대체할 수 있어.   \n",
       "3  총 몇 학점 들어야 해?  총 몇 학점 들어야 해        총 130학점 이상 이수해야 졸업할 수 있어.   \n",
       "4    교양필수 과목 알려줘   교양필수 과목 알려줘   대학공통교양으로는 글쓰기, 영어, 컴퓨팅사고가 포함돼.   \n",
       "\n",
       "                          A_clean  \n",
       "0           졸업요건을 알려면 학번과 학과를 알려줘  \n",
       "1         어학 기준은 학번별로 달라 학번을 알려줄래  \n",
       "2  논문 대신 졸업 프로젝트나 포트폴리오로 대체할 수 있어  \n",
       "3        총 130학점 이상 이수해야 졸업할 수 있어  \n",
       "4     대학공통교양으로는 글쓰기 영어 컴퓨팅사고가 포함돼  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 전처리를 해보자.\n",
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"gradu_qna_dummy.csv\", encoding = 'euc-kr')\n",
    "#print(df)\n",
    "\n",
    "# Drop rows with missing Q or A\n",
    "df.dropna(subset=[\"Q\", \"A\"], inplace=True)  # inplace=True  ==> 복사본 만들지 말고 원본으로\n",
    "\n",
    "# Basic text cleaning function\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"[^가-힣0-9a-zA-Z\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "df[\"Q_clean\"] = df[\"Q\"].apply(clean_text)\n",
    "df[\"A_clean\"] = df[\"A\"].apply(clean_text)\n",
    "\n",
    "# Show a sample\n",
    "df[[\"Q\", \"Q_clean\", \"A\", \"A_clean\"]].head()\n",
    "\n",
    "#questions = df['Q'].astype(str).tolist()\n",
    "#answers = df['A'].astype(str).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa39d62e-6666-4b14-ae07-c2b4f0140aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['졸업요건 알려줘', '어학 기준이 궁금해', '논문 꼭 써야 해 ?', '총 몇 학점 들어야 해 ?', '교양필수 과목 알려줘']\n",
      "['졸업요건을 알려면 학번과 학과를 알려줘 .', '어학 기준은 학번별로 달라 .  학번을 알려줄래 ?', '논문 대신 졸업 프로젝트나 포트폴리오로 대체할 수 있어 .', '총 130학점 이상 이수해야 졸업할 수 있어 .', '대학공통교양으로는 글쓰기 ,  영어 ,  컴퓨팅사고가 포함돼 .']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "train_data = pd.read_csv(\"gradu_qna_dummy.csv\", encoding = 'euc-kr')\n",
    "\n",
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)\n",
    "\n",
    "print(questions[:5])\n",
    "print(answers[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa19ef5-b1d9-47cd-a7dd-06acafda43f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 토큰 번호 : [416]\n",
      "종료 토큰 번호 : [417]\n",
      "단어 집합의 크기 : 418\n",
      "나는 2021학번이고 데이터사이언스학부야\n"
     ]
    }
   ],
   "source": [
    "# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "\n",
    "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "#리스트 형태로 만들어둔 이유는 나중에 input + START_TOKEN + END_TOKEN 같이 쉽게 붙이기 위해서.\n",
    "\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "print('시작 토큰 번호 :',START_TOKEN)\n",
    "print('종료 토큰 번호 :',END_TOKEN)\n",
    "print('단어 집합의 크기 :',VOCAB_SIZE)\n",
    "\n",
    "print(questions[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e529770-15a8-43ea-85a3-46e163c33e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의의 질문 샘플을 정수 인코딩 : [23, 80, 69]\n",
      "정수 인코딩 후의 문장 [23, 80, 69]\n",
      "기존 문장: 나는 2021학번이고 데이터사이언스학부야\n",
      "\n",
      "23 ----> 나는 \n",
      "80 ----> 2021학번이고 \n",
      "69 ----> 데이터사이언스학부야\n"
     ]
    }
   ],
   "source": [
    "#정수 인코딩과 패딩 시험용\n",
    "\n",
    "# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\n",
    "print('임의의 질문 샘플을 정수 인코딩 : {}'.format(tokenizer.encode(questions[20])))\n",
    "\n",
    "\n",
    "# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\n",
    "# 임의의 입력 문장을 sample_string에 저장\n",
    "sample_string = questions[20]\n",
    "\n",
    "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}\\n'.format(original_string))\n",
    "# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\n",
    "# 임의의 입력 문장을 sample_string에 저장\n",
    "\n",
    "\n",
    "# 각 정수는 각 단어와 어떻게 mapping되는지 병렬로 출력\n",
    "# 서브워드텍스트인코더는 의미있는 단위의 서브워드로 토크나이징한다. 띄어쓰기 단위 X 형태소 분석 단위 X\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de979678-1d96-49cc-80eb-d63f5277e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 데이터의 크기(shape) : (95, 40)\n",
      "답변 데이터의 크기(shape) : (95, 40)\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "\n",
    "\n",
    "print('질문 데이터의 크기(shape) :', questions.shape)\n",
    "print('답변 데이터의 크기(shape) :', answers.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432d7d94-1cc6-440d-ac16-31354fbf4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더의 입력은 답변 시퀀스에서 마지막 토큰을 제외한 것\n",
    "decoder_inputs = answers[:, :-1]\n",
    "\n",
    "# 디코더의 정답은 첫 번째 토큰을 제외한 것 (한 칸 오른쪽으로 shift)\n",
    "decoder_targets = answers[:, 1:]\n",
    "\n",
    "#encoder_input, decoder_input, decoder_target 이 트리플 세트로 묶여서, 각 문장(샘플)마다 하나씩 학습이 진행되는 구조야.\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    (questions, decoder_inputs),  # input = (encoder_input, decoder_input)\n",
    "    decoder_targets\n",
    "))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000  #일반적으로는 데이터 전체 수 이상을 쓰는 게 좋아!\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)  #데이터를 무작위로 섞어서 과적합을 줄이고 일반화 성능을 높임.\n",
    "dataset = dataset.batch(BATCH_SIZE)     #BATCH_SIZE 단위로 데이터를 묶어 모델에 전달\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)   #학습 중에 다음 데이터를 미리 준비해서 GPU가 쉬지 않게 함\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 인코더가 질문을 보고 문맥 파악\n",
    "# encoder_output = encoder(encoder_input)\n",
    "\n",
    "# 2. 디코더가 decoder_input을 보고 다음 단어를 예측\n",
    "# decoder_output = decoder(decoder_input, encoder_output)\n",
    "\n",
    "# 3. decoder_output을 final dense layer 통과 → 예측값들 (확률 분포)\n",
    "#    예측값 vs decoder_target 비교 → loss 계산 → backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f838fa-7c02-4562-95cd-d2f068d544f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=2,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    dff=512,\n",
    "    input_vocab_size=VOCAB_SIZE,\n",
    "    target_vocab_size=VOCAB_SIZE,\n",
    "    pe_input=MAX_LENGTH,\n",
    "    pe_target=MAX_LENGTH,\n",
    "    rate=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf6a8449-6213-4465-abd6-72d091fe5be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 194ms/step - accuracy_function: 0.9684 - loss: 0.0989\n",
      "Epoch 2/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy_function: 0.9673 - loss: 0.0900\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy_function: 0.9727 - loss: 0.0995\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy_function: 0.9718 - loss: 0.0997\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy_function: 0.9735 - loss: 0.0964\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - accuracy_function: 0.9712 - loss: 0.1026\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9700 - loss: 0.0954\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy_function: 0.9674 - loss: 0.0957\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy_function: 0.9734 - loss: 0.0937\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - accuracy_function: 0.9726 - loss: 0.0972\n",
      "Epoch 11/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - accuracy_function: 0.9717 - loss: 0.0946\n",
      "Epoch 12/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step - accuracy_function: 0.9718 - loss: 0.0972\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy_function: 0.9718 - loss: 0.0935\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - accuracy_function: 0.9702 - loss: 0.0974\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step - accuracy_function: 0.9751 - loss: 0.0939\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 147ms/step - accuracy_function: 0.9728 - loss: 0.0968\n",
      "Epoch 17/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9701 - loss: 0.0994\n",
      "Epoch 18/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step - accuracy_function: 0.9709 - loss: 0.0941\n",
      "Epoch 19/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy_function: 0.9692 - loss: 0.0945\n",
      "Epoch 20/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy_function: 0.9726 - loss: 0.0964\n",
      "Epoch 21/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy_function: 0.9700 - loss: 0.0961\n",
      "Epoch 22/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy_function: 0.9659 - loss: 0.0952\n",
      "Epoch 23/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy_function: 0.9728 - loss: 0.0967\n",
      "Epoch 24/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy_function: 0.9743 - loss: 0.0939\n",
      "Epoch 25/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy_function: 0.9743 - loss: 0.0944\n",
      "Epoch 26/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy_function: 0.9735 - loss: 0.0943\n",
      "Epoch 27/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - accuracy_function: 0.9692 - loss: 0.0961\n",
      "Epoch 28/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy_function: 0.9752 - loss: 0.0943\n",
      "Epoch 29/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319ms/step - accuracy_function: 0.9735 - loss: 0.0960\n",
      "Epoch 30/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy_function: 0.9683 - loss: 0.0929\n",
      "Epoch 31/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - accuracy_function: 0.9735 - loss: 0.0929\n",
      "Epoch 32/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294ms/step - accuracy_function: 0.9734 - loss: 0.0932\n",
      "Epoch 33/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy_function: 0.9797 - loss: 0.0898\n",
      "Epoch 34/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy_function: 0.9664 - loss: 0.0913\n",
      "Epoch 35/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step - accuracy_function: 0.9692 - loss: 0.0906\n",
      "Epoch 36/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy_function: 0.9735 - loss: 0.0923\n",
      "Epoch 37/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9709 - loss: 0.0928\n",
      "Epoch 38/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - accuracy_function: 0.9744 - loss: 0.0864\n",
      "Epoch 39/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - accuracy_function: 0.9751 - loss: 0.0905\n",
      "Epoch 40/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9716 - loss: 0.0861\n",
      "Epoch 41/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy_function: 0.9770 - loss: 0.0882\n",
      "Epoch 42/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - accuracy_function: 0.9726 - loss: 0.0860\n",
      "Epoch 43/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9777 - loss: 0.0871\n",
      "Epoch 44/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9771 - loss: 0.0883\n",
      "Epoch 45/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9777 - loss: 0.0881\n",
      "Epoch 46/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy_function: 0.9794 - loss: 0.0857\n",
      "Epoch 47/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9776 - loss: 0.0846\n",
      "Epoch 48/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy_function: 0.9802 - loss: 0.0848\n",
      "Epoch 49/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy_function: 0.9734 - loss: 0.0819\n",
      "Epoch 50/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9779 - loss: 0.0885\n",
      "Epoch 51/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - accuracy_function: 0.9769 - loss: 0.0875\n",
      "Epoch 52/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy_function: 0.9787 - loss: 0.0852\n",
      "Epoch 53/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy_function: 0.9747 - loss: 0.0804\n",
      "Epoch 54/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9802 - loss: 0.0864\n",
      "Epoch 55/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy_function: 0.9769 - loss: 0.0796\n",
      "Epoch 56/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy_function: 0.9726 - loss: 0.0827\n",
      "Epoch 57/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9777 - loss: 0.0828\n",
      "Epoch 58/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy_function: 0.9797 - loss: 0.0842\n",
      "Epoch 59/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy_function: 0.9786 - loss: 0.0800\n",
      "Epoch 60/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9783 - loss: 0.0831\n",
      "Epoch 61/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy_function: 0.9829 - loss: 0.0785\n",
      "Epoch 62/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy_function: 0.9785 - loss: 0.0779\n",
      "Epoch 63/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy_function: 0.9795 - loss: 0.0779\n",
      "Epoch 64/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy_function: 0.9812 - loss: 0.0773\n",
      "Epoch 65/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy_function: 0.9786 - loss: 0.0764\n",
      "Epoch 66/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy_function: 0.9744 - loss: 0.0743\n",
      "Epoch 67/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy_function: 0.9771 - loss: 0.0785\n",
      "Epoch 68/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy_function: 0.9786 - loss: 0.0727\n",
      "Epoch 69/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9828 - loss: 0.0741\n",
      "Epoch 70/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy_function: 0.9820 - loss: 0.0737\n",
      "Epoch 71/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 181ms/step - accuracy_function: 0.9820 - loss: 0.0743\n",
      "Epoch 72/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy_function: 0.9802 - loss: 0.0700\n",
      "Epoch 73/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - accuracy_function: 0.9802 - loss: 0.0741\n",
      "Epoch 74/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9804 - loss: 0.0734\n",
      "Epoch 75/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9795 - loss: 0.0743\n",
      "Epoch 76/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy_function: 0.9838 - loss: 0.0752\n",
      "Epoch 77/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9838 - loss: 0.0714\n",
      "Epoch 78/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9834 - loss: 0.0669\n",
      "Epoch 79/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 0.9811 - loss: 0.0669\n",
      "Epoch 80/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy_function: 0.9828 - loss: 0.0712\n",
      "Epoch 81/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy_function: 0.9854 - loss: 0.0693\n",
      "Epoch 82/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy_function: 0.9864 - loss: 0.0705\n",
      "Epoch 83/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9838 - loss: 0.0710\n",
      "Epoch 84/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy_function: 0.9854 - loss: 0.0703\n",
      "Epoch 85/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219ms/step - accuracy_function: 0.9889 - loss: 0.0649\n",
      "Epoch 86/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy_function: 0.9863 - loss: 0.0661\n",
      "Epoch 87/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9846 - loss: 0.0621\n",
      "Epoch 88/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy_function: 0.9829 - loss: 0.0644\n",
      "Epoch 89/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy_function: 0.9871 - loss: 0.0641\n",
      "Epoch 90/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9872 - loss: 0.0596\n",
      "Epoch 91/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9829 - loss: 0.0654\n",
      "Epoch 92/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - accuracy_function: 0.9830 - loss: 0.0640\n",
      "Epoch 93/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9855 - loss: 0.0592\n",
      "Epoch 94/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9846 - loss: 0.0611\n",
      "Epoch 95/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9905 - loss: 0.0552\n",
      "Epoch 96/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy_function: 0.9889 - loss: 0.0587\n",
      "Epoch 97/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy_function: 0.9906 - loss: 0.0589\n",
      "Epoch 98/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy_function: 0.9890 - loss: 0.0596\n",
      "Epoch 99/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9897 - loss: 0.0599\n",
      "Epoch 100/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy_function: 0.9863 - loss: 0.0605\n",
      "Epoch 101/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy_function: 0.9871 - loss: 0.0567\n",
      "Epoch 102/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy_function: 0.9872 - loss: 0.0565\n",
      "Epoch 103/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy_function: 0.9906 - loss: 0.0561\n",
      "Epoch 104/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy_function: 0.9914 - loss: 0.0559\n",
      "Epoch 105/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy_function: 0.9897 - loss: 0.0542\n",
      "Epoch 106/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 221ms/step - accuracy_function: 0.9880 - loss: 0.0528\n",
      "Epoch 107/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy_function: 0.9888 - loss: 0.0546\n",
      "Epoch 108/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy_function: 0.9898 - loss: 0.0552\n",
      "Epoch 109/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy_function: 0.9915 - loss: 0.0522\n",
      "Epoch 110/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328ms/step - accuracy_function: 0.9914 - loss: 0.0478\n",
      "Epoch 111/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy_function: 0.9889 - loss: 0.0520\n",
      "Epoch 112/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9924 - loss: 0.0512\n",
      "Epoch 113/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy_function: 0.9897 - loss: 0.0532\n",
      "Epoch 114/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - accuracy_function: 0.9941 - loss: 0.0503\n",
      "Epoch 115/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy_function: 0.9940 - loss: 0.0504\n",
      "Epoch 116/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy_function: 0.9906 - loss: 0.0505\n",
      "Epoch 117/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9922 - loss: 0.0469\n",
      "Epoch 118/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 0.9906 - loss: 0.0497\n",
      "Epoch 119/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9916 - loss: 0.0458\n",
      "Epoch 120/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy_function: 0.9941 - loss: 0.0475\n",
      "Epoch 121/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9949 - loss: 0.0460\n",
      "Epoch 122/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy_function: 0.9923 - loss: 0.0478\n",
      "Epoch 123/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - accuracy_function: 0.9940 - loss: 0.0447\n",
      "Epoch 124/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy_function: 0.9923 - loss: 0.0474\n",
      "Epoch 125/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy_function: 0.9915 - loss: 0.0445\n",
      "Epoch 126/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy_function: 0.9915 - loss: 0.0432\n",
      "Epoch 127/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy_function: 0.9941 - loss: 0.0425\n",
      "Epoch 128/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy_function: 0.9932 - loss: 0.0396\n",
      "Epoch 129/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9966 - loss: 0.0413\n",
      "Epoch 130/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 0.9966 - loss: 0.0401\n",
      "Epoch 131/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9931 - loss: 0.0426\n",
      "Epoch 132/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy_function: 0.9923 - loss: 0.0404\n",
      "Epoch 133/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 0.9940 - loss: 0.0383\n",
      "Epoch 134/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy_function: 0.9957 - loss: 0.0373\n",
      "Epoch 135/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9966 - loss: 0.0384\n",
      "Epoch 136/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy_function: 0.9957 - loss: 0.0387\n",
      "Epoch 137/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy_function: 0.9949 - loss: 0.0361\n",
      "Epoch 138/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy_function: 0.9974 - loss: 0.0370\n",
      "Epoch 139/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy_function: 0.9974 - loss: 0.0364\n",
      "Epoch 140/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy_function: 0.9957 - loss: 0.0362\n",
      "Epoch 141/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388ms/step - accuracy_function: 0.9966 - loss: 0.0384\n",
      "Epoch 142/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy_function: 0.9982 - loss: 0.0332\n",
      "Epoch 143/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9966 - loss: 0.0347\n",
      "Epoch 144/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy_function: 0.9992 - loss: 0.0334\n",
      "Epoch 145/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy_function: 0.9974 - loss: 0.0331\n",
      "Epoch 146/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy_function: 0.9966 - loss: 0.0330\n",
      "Epoch 147/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy_function: 0.9974 - loss: 0.0332\n",
      "Epoch 148/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9992 - loss: 0.0301\n",
      "Epoch 149/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy_function: 0.9957 - loss: 0.0332\n",
      "Epoch 150/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy_function: 0.9974 - loss: 0.0312\n",
      "Epoch 151/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343ms/step - accuracy_function: 0.9983 - loss: 0.0311\n",
      "Epoch 152/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy_function: 0.9966 - loss: 0.0287\n",
      "Epoch 153/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 0.9991 - loss: 0.0276\n",
      "Epoch 154/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy_function: 0.9974 - loss: 0.0268\n",
      "Epoch 155/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9966 - loss: 0.0296\n",
      "Epoch 156/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9957 - loss: 0.0279\n",
      "Epoch 157/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy_function: 0.9983 - loss: 0.0275\n",
      "Epoch 158/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - accuracy_function: 0.9967 - loss: 0.0261\n",
      "Epoch 159/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 0.9992 - loss: 0.0268\n",
      "Epoch 160/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy_function: 0.9992 - loss: 0.0275\n",
      "Epoch 161/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy_function: 0.9992 - loss: 0.0252\n",
      "Epoch 162/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - accuracy_function: 0.9974 - loss: 0.0254\n",
      "Epoch 163/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy_function: 0.9992 - loss: 0.0253\n",
      "Epoch 164/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy_function: 0.9991 - loss: 0.0253\n",
      "Epoch 165/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy_function: 0.9983 - loss: 0.0234\n",
      "Epoch 166/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy_function: 0.9982 - loss: 0.0235\n",
      "Epoch 167/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 0.9992 - loss: 0.0252\n",
      "Epoch 168/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9991 - loss: 0.0229\n",
      "Epoch 169/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy_function: 0.9992 - loss: 0.0236\n",
      "Epoch 170/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy_function: 0.9983 - loss: 0.0222\n",
      "Epoch 171/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy_function: 0.9991 - loss: 0.0223\n",
      "Epoch 172/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy_function: 0.9992 - loss: 0.0221\n",
      "Epoch 173/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy_function: 0.9983 - loss: 0.0216\n",
      "Epoch 174/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy_function: 1.0000 - loss: 0.0204\n",
      "Epoch 175/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy_function: 0.9983 - loss: 0.0222\n",
      "Epoch 176/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy_function: 0.9983 - loss: 0.0190\n",
      "Epoch 177/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy_function: 1.0000 - loss: 0.0210\n",
      "Epoch 178/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy_function: 1.0000 - loss: 0.0214\n",
      "Epoch 179/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy_function: 1.0000 - loss: 0.0195\n",
      "Epoch 180/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy_function: 0.9991 - loss: 0.0199\n",
      "Epoch 181/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy_function: 0.9983 - loss: 0.0182\n",
      "Epoch 182/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - accuracy_function: 1.0000 - loss: 0.0183\n",
      "Epoch 183/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy_function: 0.9992 - loss: 0.0178\n",
      "Epoch 184/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy_function: 0.9983 - loss: 0.0186\n",
      "Epoch 185/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265ms/step - accuracy_function: 0.9992 - loss: 0.0173\n",
      "Epoch 186/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy_function: 1.0000 - loss: 0.0170\n",
      "Epoch 187/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy_function: 1.0000 - loss: 0.0179\n",
      "Epoch 188/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - accuracy_function: 1.0000 - loss: 0.0172\n",
      "Epoch 189/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy_function: 1.0000 - loss: 0.0164\n",
      "Epoch 190/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy_function: 1.0000 - loss: 0.0154\n",
      "Epoch 191/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy_function: 0.9992 - loss: 0.0168\n",
      "Epoch 192/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy_function: 1.0000 - loss: 0.0154\n",
      "Epoch 193/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 1.0000 - loss: 0.0152\n",
      "Epoch 194/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy_function: 0.9991 - loss: 0.0150\n",
      "Epoch 195/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy_function: 1.0000 - loss: 0.0143\n",
      "Epoch 196/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy_function: 1.0000 - loss: 0.0144\n",
      "Epoch 197/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy_function: 1.0000 - loss: 0.0136\n",
      "Epoch 198/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - accuracy_function: 1.0000 - loss: 0.0129\n",
      "Epoch 199/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy_function: 1.0000 - loss: 0.0134\n",
      "Epoch 200/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy_function: 0.9991 - loss: 0.0136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f330730c80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200  # 원하는 만큼 반복\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # 학습률 계산 공식\n",
    "        \n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 패딩 마스크\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # 패딩 제외한 토큰만 계산\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "learning_rate = CustomSchedule(128)  # 네 모델에서 d_model 사용했을 거야\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    real = tf.cast(real, tf.int64)\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
    "\n",
    "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])\n",
    "\n",
    "transformer.fit(dataset, epochs=EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001946fe-c187-430b-8541-4c97a9bc7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  # 단어와 구두점 사이에 공백 추가.\n",
    "  # ex) 12시 땡! -> 12시 땡 !\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "\n",
    "def evaluate(sentence):\n",
    "  # 입력 문장에 대한 전처리\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력 문장에 시작 토큰과 종료 토큰을 추가\n",
    "  sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = transformer(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재 시점의 예측 단어를 받아온다.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 현재 시점의 예측 단어를 output(출력)에 연결한다.\n",
    "    # output은 for문의 다음 루프에서 디코더의 입력이 된다.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  # 단어 예측이 모두 끝났다면 output을 리턴.\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  # prediction == 디코더가 리턴한 챗봇의 대답에 해당하는 정수 시퀀스\n",
    "  # tokenizer.decode()를 통해 정수 시퀀스를 문자열로 디코딩.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778fc1cc-adee-44ea-b1d9-b1063e16be8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m predict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m안녕\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(sentence):\n\u001b[1;32m---> 38\u001b[0m   prediction \u001b[38;5;241m=\u001b[39m evaluate(sentence)\n\u001b[0;32m     40\u001b[0m   \u001b[38;5;66;03m# prediction == 디코더가 리턴한 챗봇의 대답에 해당하는 정수 시퀀스\u001b[39;00m\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;66;03m# tokenizer.decode()를 통해 정수 시퀀스를 문자열로 디코딩.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m   predicted_sentence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m     43\u001b[0m       [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m prediction \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mvocab_size])\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(sentence):\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;66;03m# 입력 문장에 대한 전처리\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m preprocess_sentence(sentence)\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;66;03m# 입력 문장에 시작 토큰과 종료 토큰을 추가\u001b[39;00m\n\u001b[0;32m     13\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(\n\u001b[0;32m     14\u001b[0m       START_TOKEN \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(sentence) \u001b[38;5;241m+\u001b[39m END_TOKEN, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mpreprocess_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_sentence\u001b[39m(sentence):\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;66;03m# 단어와 구두점 사이에 공백 추가.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;66;03m# ex) 12시 땡! -> 12시 땡 !\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m([?.!,])\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\"\u001b[39m, sentence)\n\u001b[0;32m      5\u001b[0m   sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sentence\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "output = predict(\"안녕\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1aa07-d92b-4707-8e2f-254c3da817d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
